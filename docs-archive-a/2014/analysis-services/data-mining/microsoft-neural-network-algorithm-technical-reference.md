---
title: Referencia técnica del algoritmo de red neuronal de Microsoft | Microsoft Docs
ms.custom: ''
ms.date: 06/13/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- HIDDEN_NODE_RATIO parameter
- MAXIMUM_INPUT_ATTRIBUTES parameter
- HOLDOUT_PERCENTAGE parameter
- neural network algorithms [Analysis Services]
- output layer [Data Mining]
- neural networks
- MAXIMUM_OUTPUT_ATTRIBUTES parameter
- MAXIMUM_STATES parameter
- SAMPLE_SIZE parameter
- hidden layer
- hidden neurons
- input layer [Data Mining]
- activation function [Data Mining]
- Back-Propagated Delta Rule network
- neural network model [Analysis Services]
- coding [Data Mining]
- HOLDOUT_SEED parameter
ms.assetid: b8fac409-e3c0-4216-b032-364f8ea51095
author: minewiskan
ms.author: owend
ms.openlocfilehash: 3c36fd9f3446ddf36da9af7ce58259edbe84c8cf
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: es-ES
ms.lasthandoff: 08/04/2020
ms.locfileid: "87662364"
---
# <a name="microsoft-neural-network-algorithm-technical-reference"></a><span data-ttu-id="324a8-102">Referencia técnica del algoritmo de red neuronal de Microsoft</span><span class="sxs-lookup"><span data-stu-id="324a8-102">Microsoft Neural Network Algorithm Technical Reference</span></span>
  <span data-ttu-id="324a8-103">La red neuronal de [!INCLUDE[msCoName](../../includes/msconame-md.md)] usa una red de tipo *perceptrón multinivel* , que también se denomina *red de tipo regla delta de propagación hacia atrás*, compuesta por tres niveles de neuronas o *perceptrones*.</span><span class="sxs-lookup"><span data-stu-id="324a8-103">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network uses a *Multilayer Perceptron* network, also called a *Back-Propagated Delta Rule network*, composed of up to three layers of neurons, or *perceptrons*.</span></span> <span data-ttu-id="324a8-104">Estos niveles son: un nivel de entrada, un nivel oculto opcional y un nivel de salida.</span><span class="sxs-lookup"><span data-stu-id="324a8-104">These layers are an input layer, an optional hidden layer, and an output layer.</span></span>  
  
 <span data-ttu-id="324a8-105">Esta documentación no abarca una discusión detallada sobre redes neuronales de tipo perceptrón multinivel.</span><span class="sxs-lookup"><span data-stu-id="324a8-105">A detailed discussion of Multilayer Perceptron neural networks is outside the scope of this documentation.</span></span> <span data-ttu-id="324a8-106">En este tema, se explica la implementación básica del algoritmo, incluido el método usado para normalizar los valores de entrada y de salida, y los métodos de selección de características usados para reducir la cardinalidad de los atributos.</span><span class="sxs-lookup"><span data-stu-id="324a8-106">This topic explains the basic implementation of the algorithm, including the method used to normalize input and output values, and feature selection methods used to reduce attribute cardinality.</span></span> <span data-ttu-id="324a8-107">En este tema, se describen los parámetros y otros valores que se pueden usar para personalizar el comportamiento del algoritmo; además, se proporcionan vínculos a información adicional sobre cómo consultar el modelo.</span><span class="sxs-lookup"><span data-stu-id="324a8-107">This topic describes the parameters and other settings that can be used to customize the behavior of the algorithm, and provides links to additional information about querying the model.</span></span>  
  
## <a name="implementation-of-the-microsoft-neural-network-algorithm"></a><span data-ttu-id="324a8-108">Implementación del algoritmo de red neuronal de Microsoft</span><span class="sxs-lookup"><span data-stu-id="324a8-108">Implementation of the Microsoft Neural Network Algorithm</span></span>  
 <span data-ttu-id="324a8-109">En una red neuronal de tipo perceptrón multinivel, cada neurona recibe una o más entradas y genera una o más salidas idénticas.</span><span class="sxs-lookup"><span data-stu-id="324a8-109">In a Multilayer Perceptron neural network, each neuron receives one or more inputs and produces one or more identical outputs.</span></span> <span data-ttu-id="324a8-110">Cada salida es una función no lineal simple de la suma de las entradas a la neurona.</span><span class="sxs-lookup"><span data-stu-id="324a8-110">Each output is a simple non-linear function of the sum of the inputs to the neuron.</span></span> <span data-ttu-id="324a8-111">Las entradas pasan de los nodos del nivel de entrada a los nodos del nivel oculto y, a continuación, al nivel de salida; no existe ninguna conexión entre neuronas del mismo nivel.</span><span class="sxs-lookup"><span data-stu-id="324a8-111">Inputs pass forward from nodes in the input layer to nodes in the hidden layer, and then pass from the hidden layer to the output layer; there are no connections between neurons within a layer.</span></span> <span data-ttu-id="324a8-112">Si no se incluye ningún nivel oculto, tal y como pasa en un modelo de regresión logística, las entradas pasan directamente desde los nodos del nivel de entrada a los nodos del nivel de salida.</span><span class="sxs-lookup"><span data-stu-id="324a8-112">If no hidden layer is included, as in a logistic regression model, inputs pass forward directly from nodes in the input layer to nodes in the output layer.</span></span>  
  
 <span data-ttu-id="324a8-113">Existen tres tipos de neuronas en una red neuronal creada con el algoritmo de red neuronal de [!INCLUDE[msCoName](../../includes/msconame-md.md)] :</span><span class="sxs-lookup"><span data-stu-id="324a8-113">There are three types of neurons in a neural network that is created with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm:</span></span>  
  
-   `Input neurons`  
  
 <span data-ttu-id="324a8-114">Las neuronas de entrada proporcionan valores de atributo de entrada para el modelo de minería de datos.</span><span class="sxs-lookup"><span data-stu-id="324a8-114">Input neurons provide input attribute values for the data mining model.</span></span> <span data-ttu-id="324a8-115">En el caso de los atributos de entrada discretos, las neuronas de entrada suelen representar un único estado del atributo de entrada.</span><span class="sxs-lookup"><span data-stu-id="324a8-115">For discrete input attributes, an input neuron typically represents a single state from the input attribute.</span></span> <span data-ttu-id="324a8-116">Esto incluye los valores ausentes, si los datos de entrenamiento contienen valores NULL para ese atributo.</span><span class="sxs-lookup"><span data-stu-id="324a8-116">This includes missing values, if the training data contains nulls for that attribute.</span></span> <span data-ttu-id="324a8-117">Un atributo de entrada discreto que tiene más de dos estados genera una neurona de entrada por cada estado y una neurona de entrada para un estado ausente, si existen valores NULL en los datos de entrenamiento.</span><span class="sxs-lookup"><span data-stu-id="324a8-117">A discrete input attribute that has more than two states generates one input neuron for each state, and one input neuron for a missing state, if there are any nulls in the training data.</span></span> <span data-ttu-id="324a8-118">Un atributo de entrada continuo genera dos neuronas de entrada: una neurona para un estado ausente y otra neurona para el valor del propio atributo continuo.</span><span class="sxs-lookup"><span data-stu-id="324a8-118">A continuous input attribute generates two input neurons: one neuron for a missing state, and one neuron for the value of the continuous attribute itself.</span></span> <span data-ttu-id="324a8-119">Las neuronas de entrada proporcionan entradas para una o más neuronas ocultas.</span><span class="sxs-lookup"><span data-stu-id="324a8-119">Input neurons provide inputs to one or more hidden neurons.</span></span>  
  
-   `Hidden neurons`  
  
 <span data-ttu-id="324a8-120">Las neuronas ocultas reciben entradas de las neuronas de entrada y proporcionan salidas a las neuronas de salida.</span><span class="sxs-lookup"><span data-stu-id="324a8-120">Hidden neurons receive inputs from input neurons and provide outputs to output neurons.</span></span>  
  
-   `Output neurons`  
  
 <span data-ttu-id="324a8-121">Las neuronas de salida representan valores de atributo de predicción para el modelo de minería de datos.</span><span class="sxs-lookup"><span data-stu-id="324a8-121">Output neurons represent predictable attribute values for the data mining model.</span></span> <span data-ttu-id="324a8-122">En el caso de los atributos de entrada discretos, una neurona de salida suele representar un único estado de predicción para un atributo de predicción, incluidos los valores que faltan.</span><span class="sxs-lookup"><span data-stu-id="324a8-122">For discrete input attributes, an output neuron typically represents a single predicted state for a predictable attribute, including missing values.</span></span> <span data-ttu-id="324a8-123">Por ejemplo, un atributo de predicción binario produce un nodo de salida que describe un estado ausente o existente, que indica si existe un valor para ese atributo.</span><span class="sxs-lookup"><span data-stu-id="324a8-123">For example, a binary predictable attribute produces one output node that describes a missing or existing state, to indicate whether a value exists for that attribute.</span></span> <span data-ttu-id="324a8-124">Una columna booleana utilizada como un atributo de predicción genera tres neuronas de salida: una neurona para un valor true, otra neurona para un valor false y una última neurona para un estado existente o ausente.</span><span class="sxs-lookup"><span data-stu-id="324a8-124">A Boolean column that is used as a predictable attribute generates three output neurons: one neuron for a true value, one neuron for a false value, and one neuron for a missing or existing state.</span></span> <span data-ttu-id="324a8-125">Un atributo de predicción discreto que tiene más de dos estados genera una neurona de salida por cada estado y una neurona de salida para un estado ausente o existente.</span><span class="sxs-lookup"><span data-stu-id="324a8-125">A discrete predictable attribute that has more than two states generates one output neuron for each state, and one output neuron for a missing or existing state.</span></span> <span data-ttu-id="324a8-126">Las columnas de predicción continuas generan dos neuronas de salida: una neurona para  un estado existente o ausente y otra neurona para el valor de la propia columna continua.</span><span class="sxs-lookup"><span data-stu-id="324a8-126">Continuous predictable columns generate two output neurons: one neuron for a missing or existing state, and one neuron for the value of the continuous column itself.</span></span> <span data-ttu-id="324a8-127">Si se generan más de 500 neuronas de salida al revisar el conjunto de columnas de predicción, [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] genera una red nueva en el modelo de minería de datos para representar las neuronas de salida adicionales.</span><span class="sxs-lookup"><span data-stu-id="324a8-127">If more than 500 output neurons are generated by reviewing the set of predictable columns, [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] generates a new network in the mining model to represent the additional output neurons.</span></span>  
  
 <span data-ttu-id="324a8-128">Una neurona recibe la entrada de otras neuronas o de otros datos, dependiendo del nivel de la red en que se encuentra.</span><span class="sxs-lookup"><span data-stu-id="324a8-128">A neuron receives input from other neurons, or from other data, depending on which layer of the network it is in.</span></span> <span data-ttu-id="324a8-129">Una neurona de entrada recibe entradas de los datos originales.</span><span class="sxs-lookup"><span data-stu-id="324a8-129">An input neuron receives inputs from the original data.</span></span> <span data-ttu-id="324a8-130">Las neuronas ocultas y las neuronas de salida reciben entradas de la salida de otras neuronas de la red neuronal.</span><span class="sxs-lookup"><span data-stu-id="324a8-130">Hidden neurons and output neurons receive inputs from the output of other neurons in the neural network.</span></span> <span data-ttu-id="324a8-131">Las entradas establecen relaciones entre neuronas; estas relaciones sirven como ruta de análisis para un conjunto específico de escenarios.</span><span class="sxs-lookup"><span data-stu-id="324a8-131">Inputs establish relationships between neurons, and the relationships serve as a path of analysis for a specific set of cases.</span></span>  
  
 <span data-ttu-id="324a8-132">Cada entrada tiene un valor asignado denominado *peso*, que describe la relevancia o importancia de dicha entrada en la neurona oculta o en la neurona de salida.</span><span class="sxs-lookup"><span data-stu-id="324a8-132">Each input has a value assigned to it, called the *weight*, which describes the relevance or importance of that particular input to the hidden neuron or the output neuron.</span></span> <span data-ttu-id="324a8-133">Cuanto mayor sea el peso asignado a una entrada, más importante o relevante será el valor de dicha entrada.</span><span class="sxs-lookup"><span data-stu-id="324a8-133">The greater the weight that is assigned to an input, the more relevant or important the value of that input.</span></span> <span data-ttu-id="324a8-134">Los pesos pueden ser negativos, lo cual implica que la entrada puede impedir, en lugar de activar, una neurona específica.</span><span class="sxs-lookup"><span data-stu-id="324a8-134">Weights can be negative, which implies that the input can inhibit, rather than activate, a specific neuron.</span></span> <span data-ttu-id="324a8-135">El valor de cada entrada se multiplica por el peso para poner de relieve la importancia de la entrada de una neurona específica.</span><span class="sxs-lookup"><span data-stu-id="324a8-135">The value of each input is multiplied by the weight to emphasize the importance of an input for a specific neuron.</span></span> <span data-ttu-id="324a8-136">En el caso de pesos negativos, el efecto de multiplicar el valor por el peso es una pérdida de importancia.</span><span class="sxs-lookup"><span data-stu-id="324a8-136">For negative weights, the effect of multiplying the value by the weight is to deemphasize the importance.</span></span>  
  
 <span data-ttu-id="324a8-137">Cada neurona tiene una función no lineal sencilla asignada denominada *función de activación*que describe la relevancia o importancia de una neurona específica para ese nivel de una red neuronal.</span><span class="sxs-lookup"><span data-stu-id="324a8-137">Each neuron has a simple non-linear function assigned to it, called the *activation function*, which describes the relevance or importance of a particular neuron to that layer of a neural network.</span></span> <span data-ttu-id="324a8-138">Las neuronas ocultas usan una función *tangente hiperbólica* (tanh) para su función de activación, mientras que las neuronas de salida usan una función *sigmoidea* para la activación.</span><span class="sxs-lookup"><span data-stu-id="324a8-138">Hidden neurons use a *hyperbolic tangent* function (tanh) for their activation function, whereas output neurons use a *sigmoid* function for activation.</span></span> <span data-ttu-id="324a8-139">Ambas son funciones no lineales continuas que permiten que la red neuronal modele relaciones no lineales entre neuronas de entrada y salida.</span><span class="sxs-lookup"><span data-stu-id="324a8-139">Both functions are nonlinear, continuous functions that allow the neural network to model nonlinear relationships between input and output neurons.</span></span>  
  
### <a name="training-neural-networks"></a><span data-ttu-id="324a8-140">Redes neuronales de entrenamiento</span><span class="sxs-lookup"><span data-stu-id="324a8-140">Training Neural Networks</span></span>  
 <span data-ttu-id="324a8-141">Existen varios pasos implicados en el entrenamiento de un modelo de minería de datos que utiliza el algoritmo de red neuronal de [!INCLUDE[msCoName](../../includes/msconame-md.md)] .</span><span class="sxs-lookup"><span data-stu-id="324a8-141">Several steps are involved in training a data mining model that uses the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span> <span data-ttu-id="324a8-142">Estos pasos están muy influenciados por los valores que se especifican en los parámetros de algoritmo.</span><span class="sxs-lookup"><span data-stu-id="324a8-142">These steps are heavily influenced by the values that you specify for the algorithm parameters.</span></span>  
  
 <span data-ttu-id="324a8-143">En primer lugar, el algoritmo evalúa y extrae los datos de entrenamiento del origen de datos.</span><span class="sxs-lookup"><span data-stu-id="324a8-143">The algorithm first evaluates and extracts training data from the data source.</span></span> <span data-ttu-id="324a8-144">Un porcentaje de los datos de entrenamiento, denominado *datos de exclusión*, se reserva para evaluar la precisión de la red.</span><span class="sxs-lookup"><span data-stu-id="324a8-144">A percentage of the training data, called the *holdout data*, is reserved for use in assessing the accuracy of the network.</span></span> <span data-ttu-id="324a8-145">Durante el proceso de entrenamiento, la red se evalúa de forma inmediata después de cada iteración mediante los datos de entrenamiento.</span><span class="sxs-lookup"><span data-stu-id="324a8-145">Throughout the training process, the network is evaluated immediately after each iteration through the training data.</span></span> <span data-ttu-id="324a8-146">Cuando la precisión deja de aumentar, el proceso de entrenamiento se detiene.</span><span class="sxs-lookup"><span data-stu-id="324a8-146">When the accuracy no longer increases, the training process is stopped.</span></span>  
  
 <span data-ttu-id="324a8-147">Los valores de los parámetros *SAMPLE_SIZE* y *HOLDOUT_PERCENTAGE* se usan para determinar el número de casos de muestra de los datos de aprendizaje y el número de casos que se apartan en los datos de exclusión.</span><span class="sxs-lookup"><span data-stu-id="324a8-147">The values of the *SAMPLE_SIZE* and *HOLDOUT_PERCENTAGE* parameters are used to determine the number of cases to sample from the training data and the number of cases to be put aside for the holdout data.</span></span> <span data-ttu-id="324a8-148">El valor del parámetro *HOLDOUT_SEED* se usa para determinar de forma aleatoria los casos individuales que se apartan para los datos de exclusión.</span><span class="sxs-lookup"><span data-stu-id="324a8-148">The value of the *HOLDOUT_SEED* parameter is used to randomly determine the individual cases to be put aside for the holdout data.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="324a8-149">Estos parámetros de algoritmo son diferentes de las propiedades HOLDOUT_SEED y HOLDOUT_SIZE, que se aplican a una estructura de minería de datos para definir un conjunto de datos de prueba.</span><span class="sxs-lookup"><span data-stu-id="324a8-149">These algorithm parameters are different from the HOLDOUT_SIZE and HOLDOUT_SEED properties, which are applied to a mining structure to define a testing data set.</span></span>  
  
 <span data-ttu-id="324a8-150">A continuación, el algoritmo determina el número y la complejidad de las redes que admite el modelo de minería de datos.</span><span class="sxs-lookup"><span data-stu-id="324a8-150">The algorithm next determines the number and complexity of the networks that the mining model supports.</span></span> <span data-ttu-id="324a8-151">Si el modelo contiene uno o más atributos que solamente se utilizan para la predicción, el algoritmo crea una única red que representa todos estos atributos.</span><span class="sxs-lookup"><span data-stu-id="324a8-151">If the mining model contains one or more attributes that are used only for prediction, the algorithm creates a single network that represents all such attributes.</span></span> <span data-ttu-id="324a8-152">Si el modelo de minería de datos contiene uno o más atributos que se utilizan para la entrada y la predicción, el proveedor del algoritmo construye una red para cada atributo.</span><span class="sxs-lookup"><span data-stu-id="324a8-152">If the mining model contains one or more attributes that are used for both input and prediction, the algorithm provider constructs a network for each attribute.</span></span>  
  
 <span data-ttu-id="324a8-153">En el caso de los atributos de entrada y de predicción que tienen valores discretos, cada neurona de entrada o de salida representa respectivamente un único estado.</span><span class="sxs-lookup"><span data-stu-id="324a8-153">For input and predictable attributes that have discrete values, each input or output neuron respectively represents a single state.</span></span> <span data-ttu-id="324a8-154">En el caso de los atributos de entrada y de predicción que tienen valores continuos, cada neurona de entrada o de salida representa respectivamente el intervalo y la distribución de valores del atributo.</span><span class="sxs-lookup"><span data-stu-id="324a8-154">For input and predictable attributes that have continuous values, each input or output neuron respectively represents the range and distribution of values for the attribute.</span></span> <span data-ttu-id="324a8-155">El número máximo de estados admitidos en cada caso depende del valor del parámetro de algoritmo *MAXIMUM_STATES* .</span><span class="sxs-lookup"><span data-stu-id="324a8-155">The maximum number of states that is supported in either case depends on the value of the *MAXIMUM_STATES* algorithm parameter.</span></span> <span data-ttu-id="324a8-156">Si el número de estados para un atributo específico supera el valor del parámetro de algoritmo *MAXIMUM_STATES* , se eligen los estados más comunes o relevantes para dicho atributo hasta alcanzar el número máximo permitido y el resto de los estados se agrupan como valores ausentes para el análisis.</span><span class="sxs-lookup"><span data-stu-id="324a8-156">If the number of states for a specific attribute exceeds the value of the *MAXIMUM_STATES* algorithm parameter, the most popular or relevant states for that attribute are chosen, up to the maximum number of states allowed, and the remaining states are grouped as missing values for the purposes of analysis.</span></span>  
  
 <span data-ttu-id="324a8-157">Después, el algoritmo usa el valor del parámetro *HIDDEN_NODE_RATIO* al determinar el número inicial de neuronas que se crearán para el nivel oculto.</span><span class="sxs-lookup"><span data-stu-id="324a8-157">The algorithm then uses the value of the *HIDDEN_NODE_RATIO* parameter when determining the initial number of neurons to create for the hidden layer.</span></span> <span data-ttu-id="324a8-158">Puede establecer *HIDDEN_NODE_RATIO* en 0 para evitar la creación de un nivel oculto en las redes que genera el algoritmo para el modelo de minería de datos y tratar la red neuronal como una regresión logística.</span><span class="sxs-lookup"><span data-stu-id="324a8-158">You can set *HIDDEN_NODE_RATIO* to 0 to prevent the creation of a hidden layer in the networks that the algorithm generates for the mining model, to treat the neural network as a logistic regression.</span></span>  
  
 <span data-ttu-id="324a8-159">El proveedor de algoritmos evalúa iterativamente el peso de todas las entradas de la red simultáneamente, tomando el conjunto de datos de entrenamiento reservado anteriormente y comparando el valor real conocido de cada escenario de los datos de exclusión con la predicción de la red, en un proceso conocido como *aprendizaje por lotes*.</span><span class="sxs-lookup"><span data-stu-id="324a8-159">The algorithm provider iteratively evaluates the weight for all inputs across the network at the same time, by taking the set of training data that was reserved earlier and comparing the actual known value for each case in the holdout data with the network's prediction, in a process known as *batch learning*.</span></span> <span data-ttu-id="324a8-160">Una vez que el algoritmo ha evaluado el conjunto completo de los datos de entrenamiento, revisa el valor predicho y real de cada neurona.</span><span class="sxs-lookup"><span data-stu-id="324a8-160">After the algorithm has evaluated the entire set of training data, the algorithm reviews the predicted and actual value for each neuron.</span></span> <span data-ttu-id="324a8-161">El algoritmo calcula el grado de error, si lo hay, y ajusta los pesos asociados con las entradas de esa neurona, trabajando hacia atrás desde las neuronas de salida a las de entrada en un proceso conocido como *propagación hacia atrás*.</span><span class="sxs-lookup"><span data-stu-id="324a8-161">The algorithm calculates the degree of error, if any, and adjusts the weights that are associated with the inputs for that neuron, working backward from output neurons to input neurons in a process known as *backpropagation*.</span></span> <span data-ttu-id="324a8-162">A continuación, el algoritmo repite el proceso en todo el conjunto de datos de entrenamiento.</span><span class="sxs-lookup"><span data-stu-id="324a8-162">The algorithm then repeats the process over the entire set of training data.</span></span> <span data-ttu-id="324a8-163">Dado que el algoritmo puede admitir múltiples pesos y neuronas de salida, el algoritmo de gradiente conjugado se utiliza para guiar el proceso de entrenamiento en la asignación y evaluación de los pesos de las entradas.</span><span class="sxs-lookup"><span data-stu-id="324a8-163">Because the algorithm can support many weights and output neurons, the conjugate gradient algorithm is used to guide the training process for assigning and evaluating weights for inputs.</span></span> <span data-ttu-id="324a8-164">Esta documentación no abarca una discusión sobre el algoritmo de gradiente conjugado.</span><span class="sxs-lookup"><span data-stu-id="324a8-164">A discussion of the conjugate gradient algorithm is outside the scope of this documentation.</span></span>  
  
### <a name="feature-selection"></a><span data-ttu-id="324a8-165">Selección de características</span><span class="sxs-lookup"><span data-stu-id="324a8-165">Feature Selection</span></span>  
 <span data-ttu-id="324a8-166">Si el número de atributos de entrada es mayor que el valor del parámetro *MAXIMUM_INPUT_ATTRIBUTES* (o si el número de atributos de predicción es mayor que el valor del parámetro *MAXIMUM_OUTPUT_ATTRIBUTES* ), se usa un algoritmo de selección de características para reducir la complejidad de las redes que se incluyen en el modelo de minería de datos.</span><span class="sxs-lookup"><span data-stu-id="324a8-166">If the number of input attributes is greater than the value of the *MAXIMUM_INPUT_ATTRIBUTES* parameter, or if the number of predictable attributes is greater than the value of the *MAXIMUM_OUTPUT_ATTRIBUTES* parameter, a feature selection algorithm is used to reduce the complexity of the networks that are included in the mining model.</span></span> <span data-ttu-id="324a8-167">La selección de características reduce el número de atributos de entrada o de predicción a los más relevantes estadísticamente para el modelo.</span><span class="sxs-lookup"><span data-stu-id="324a8-167">Feature selection reduces the number of input or predictable attributes to those that are most statistically relevant to the model.</span></span>  
  
 <span data-ttu-id="324a8-168">Todos los algoritmos de minería de datos de [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] usan automáticamente la selección de características para mejorar el análisis y reducir la carga de procesamiento.</span><span class="sxs-lookup"><span data-stu-id="324a8-168">Feature selection is used automatically by all [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] data mining algorithms to improve analysis and reduce processing load.</span></span> <span data-ttu-id="324a8-169">El método usado para la selección de características en los modelos de red neuronal depende del tipo de datos del atributo.</span><span class="sxs-lookup"><span data-stu-id="324a8-169">The method used for feature selection in neural network models depends on the data type of the attribute.</span></span> <span data-ttu-id="324a8-170">Como referencia, en la tabla siguiente se muestran los métodos de selección de características usados para los modelos de red neuronal; además, se muestran los métodos de selección de características usados para el algoritmo de regresión logística, que está basado en el algoritmo de red neuronal.</span><span class="sxs-lookup"><span data-stu-id="324a8-170">For reference, the following table shows the feature selection methods used for neural network models, and also shows the feature selection methods used for the Logistic Regression algorithm, which is based on the Neural Network algorithm.</span></span>  
  
|<span data-ttu-id="324a8-171">Algoritmo</span><span class="sxs-lookup"><span data-stu-id="324a8-171">Algorithm</span></span>|<span data-ttu-id="324a8-172">Método de análisis</span><span class="sxs-lookup"><span data-stu-id="324a8-172">Method of analysis</span></span>|<span data-ttu-id="324a8-173">Comentarios</span><span class="sxs-lookup"><span data-stu-id="324a8-173">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="324a8-174">Red neuronal</span><span class="sxs-lookup"><span data-stu-id="324a8-174">Neural Network</span></span>|<span data-ttu-id="324a8-175">Puntuación interestingness</span><span class="sxs-lookup"><span data-stu-id="324a8-175">Interestingness score</span></span><br /><br /> <span data-ttu-id="324a8-176">Entropía de Shannon</span><span class="sxs-lookup"><span data-stu-id="324a8-176">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="324a8-177">Bayesiano con prioridad K2</span><span class="sxs-lookup"><span data-stu-id="324a8-177">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="324a8-178">Dirichlet bayesiano con prioridad uniforme (predeterminado)</span><span class="sxs-lookup"><span data-stu-id="324a8-178">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="324a8-179">El algoritmo de redes neuronales puede usar ambos métodos de puntuación, el bayesiano y el basado en la entropía, siempre y cuando los datos contengan columnas continuas.</span><span class="sxs-lookup"><span data-stu-id="324a8-179">The Neural Networks algorithm can use both entropy-based and Bayesian scoring methods, as long as the data contains continuous columns.</span></span><br /><br /> <span data-ttu-id="324a8-180">Predeterminado</span><span class="sxs-lookup"><span data-stu-id="324a8-180">Default.</span></span>|  
|<span data-ttu-id="324a8-181">Regresión logística</span><span class="sxs-lookup"><span data-stu-id="324a8-181">Logistic Regression</span></span>|<span data-ttu-id="324a8-182">Puntuación interestingness</span><span class="sxs-lookup"><span data-stu-id="324a8-182">Interestingness score</span></span><br /><br /> <span data-ttu-id="324a8-183">Entropía de Shannon</span><span class="sxs-lookup"><span data-stu-id="324a8-183">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="324a8-184">Bayesiano con prioridad K2</span><span class="sxs-lookup"><span data-stu-id="324a8-184">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="324a8-185">Dirichlet bayesiano con prioridad uniforme (predeterminado)</span><span class="sxs-lookup"><span data-stu-id="324a8-185">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="324a8-186">Dado que no se puede pasar un parámetro a este algoritmo para controlar el comportamiento de la selección de características, se utilizan los valores predeterminados.</span><span class="sxs-lookup"><span data-stu-id="324a8-186">Because you cannot pass a parameter to this algorithm to control feature election behavior, the defaults are used.</span></span> <span data-ttu-id="324a8-187">Por consiguiente, si todos los atributos son discretos o contienen datos discretos, el valor predeterminado es BDEU.</span><span class="sxs-lookup"><span data-stu-id="324a8-187">Therefore, if all attributes are discrete or discretized, the default is BDEU.</span></span>|  
  
 <span data-ttu-id="324a8-188">Los parámetros de algoritmo que controlan la selección de características para un modelo de red neuronal son MAXIMUM_INPUT_ATTRIBUTES, MAXIMUM_OUTPUT_ATTRIBUTES y MAXIMUM_STATES.</span><span class="sxs-lookup"><span data-stu-id="324a8-188">The algorithm parameters that control feature selection for a neural network model are MAXIMUM_INPUT_ATTRIBUTES, MAXIMUM_OUTPUT_ATTRIBUTES, and MAXIMUM_STATES.</span></span> <span data-ttu-id="324a8-189">También puede controlar el número de niveles ocultos mediante el establecimiento del parámetro HIDDEN_NODE_RATIO.</span><span class="sxs-lookup"><span data-stu-id="324a8-189">You can also control the number of hidden layers by setting the HIDDEN_NODE_RATIO parameter.</span></span>  
  
### <a name="scoring-methods"></a><span data-ttu-id="324a8-190">Métodos de puntuación</span><span class="sxs-lookup"><span data-stu-id="324a8-190">Scoring Methods</span></span>  
 <span data-ttu-id="324a8-191">La*puntuación* es un tipo de normalización que, en el contexto del entrenamiento de un modelo de red neuronal, hace referencia al proceso de convertir un valor, como una etiqueta de texto discreta, en un valor que se pueda comparar con otros tipos de entradas y que se pueda pesar en la red.</span><span class="sxs-lookup"><span data-stu-id="324a8-191">*Scoring* is a kind of normalization, which in the context of training a neural network model means the process of converting a value, such as a discrete text label, into a value that can be compared with other types of inputs and weighted in the network.</span></span> <span data-ttu-id="324a8-192">Por ejemplo, si un atributo de entrada es Sexo y los valores posibles son Hombre y Mujer, y otro atributo de entrada es Ingresos, con un intervalo de valores variable, los valores para cada atributo no son comparables directamente y, por consiguiente, deben estar codificados a una escala común para que se puedan calcular los pesos.</span><span class="sxs-lookup"><span data-stu-id="324a8-192">For example, if one input attribute is Gender and the possible values are Male and Female, and another input attribute is Income, with a variable range of values, the values for each attribute are not directly comparable, and therefore must be encoded to a common scale so that the weights can be computed.</span></span> <span data-ttu-id="324a8-193">Puntuar es el proceso de normalizar tales entradas para los valores numéricos, específicamente, para un intervalo de probabilidades.</span><span class="sxs-lookup"><span data-stu-id="324a8-193">Scoring is the process of normalizing such inputs to numeric values: specifically, to a probability range.</span></span> <span data-ttu-id="324a8-194">Las funciones usadas para la normalización también ayudan a distribuir más uniformemente los valores de entrada en una escala uniforme para que los valores extremos no distorsionen los resultados del análisis.</span><span class="sxs-lookup"><span data-stu-id="324a8-194">The functions used for normalization also help to distribute input value more evenly on a uniform scale so that extreme values do not distort the results of analysis.</span></span>  
  
 <span data-ttu-id="324a8-195">Las salidas de la red neuronal también están codificadas.</span><span class="sxs-lookup"><span data-stu-id="324a8-195">Outputs of the neural network are also encoded.</span></span> <span data-ttu-id="324a8-196">Si hay un único destino para la salida (es decir, la predicción), o varios destinos que se usan solo para la predicción, no para la entrada, el modelo crea una red única y es posible que no sea necesario normalizar los valores.</span><span class="sxs-lookup"><span data-stu-id="324a8-196">When there is a single target for output (that is, prediction), or multiple targets that are used for prediction only and not for input, the model create a single network and it might not seem necessary to normalize the values.</span></span> <span data-ttu-id="324a8-197">Sin embargo, si se usan varios atributos para la entrada y la predicción, el modelo debe crear varias redes; por tanto, se deben normalizar todos los valores y, al salir de la red, las salidas deberán estar codificadas.</span><span class="sxs-lookup"><span data-stu-id="324a8-197">However, if multiple attributes are used for input and prediction, the model must create multiple networks; therefore, all values must be normalized, and the outputs too must be encoded as they exit the network.</span></span>  
  
 <span data-ttu-id="324a8-198">La codificación de las entradas se basa en la suma de cada valor discreto de los casos de entrenamiento y en la multiplicación de ese valor por su peso.</span><span class="sxs-lookup"><span data-stu-id="324a8-198">Encoding for inputs is based on summing each discrete value in the training cases, and multiplying that value by its weight.</span></span> <span data-ttu-id="324a8-199">Esto se denomina *suma ponderada*, que se pasa a la función de activación del nivel oculto.</span><span class="sxs-lookup"><span data-stu-id="324a8-199">This is called a *weighted sum*, which is passed to the activation function in the hidden layer.</span></span> <span data-ttu-id="324a8-200">Para la codificación, se usa la puntuación-z:</span><span class="sxs-lookup"><span data-stu-id="324a8-200">A z-score is used for encoding, as follows:</span></span>  
  
 <span data-ttu-id="324a8-201">**Valores discretos**</span><span class="sxs-lookup"><span data-stu-id="324a8-201">**Discrete values**</span></span>  
  
 <span data-ttu-id="324a8-202">μ = p: la probabilidad anterior de un estado</span><span class="sxs-lookup"><span data-stu-id="324a8-202">μ = p - the prior probability of a state</span></span>  
  
 <span data-ttu-id="324a8-203">StdDev = sqrt(p(1-p))</span><span class="sxs-lookup"><span data-stu-id="324a8-203">StdDev  = sqrt(p(1-p))</span></span>  
  
 <span data-ttu-id="324a8-204">**Valores continuos**</span><span class="sxs-lookup"><span data-stu-id="324a8-204">**Continuous values**</span></span>  
  
 <span data-ttu-id="324a8-205">Valor presente = 1-μ/σ</span><span class="sxs-lookup"><span data-stu-id="324a8-205">Value present= 1 - μ/σ</span></span>  
  
 <span data-ttu-id="324a8-206">Ningún valor existente =-μ/σ</span><span class="sxs-lookup"><span data-stu-id="324a8-206">No existing value= -μ/σ</span></span>  
  
 <span data-ttu-id="324a8-207">Una vez codificados los valores, se realiza una suma ponderada de las entradas, con los extremos de la red como pesos.</span><span class="sxs-lookup"><span data-stu-id="324a8-207">After the values have been encoded, the inputs go through weighted summing, with network edges as weights.</span></span>  
  
 <span data-ttu-id="324a8-208">La codificación de las salidas usa la función sigmoidea, que tiene propiedades que la hacen muy útil para la predicción.</span><span class="sxs-lookup"><span data-stu-id="324a8-208">Encoding for outputs uses the sigmoid function, which has properties that make it very useful for prediction.</span></span> <span data-ttu-id="324a8-209">Una de esas propiedades es que, sin tener en cuenta cómo se ajusta la escala de los valores originales, y sin tener en cuenta si los valores son negativos o positivos, la salida de esta función es siempre un valor entre 0 y 1, lo que resulta apropiado para la estimación de probabilidades.</span><span class="sxs-lookup"><span data-stu-id="324a8-209">One such property is that, regardless of how the original values are scaled, and regardless of whether values are negative or positive, the output of this function is always a value between 0 and 1, which is suited for estimating probabilities.</span></span> <span data-ttu-id="324a8-210">Otra propiedad útil es que la función sigmoidea tiene un efecto suavizador que hace que cuando los valores se alejan del punto de inflexión, la probabilidad del valor se aproxima lentamente a 0 o a 1.</span><span class="sxs-lookup"><span data-stu-id="324a8-210">Another useful property is that the sigmoid function has a smoothing effect, so that as values move farther away from point of inflection, the probability for the value moves towards 0 or 1, but slowly.</span></span>  
  
## <a name="customizing-the-neural-network-algorithm"></a><span data-ttu-id="324a8-211">Personalizar el algoritmo de red neuronal</span><span class="sxs-lookup"><span data-stu-id="324a8-211">Customizing the Neural Network Algorithm</span></span>  
 <span data-ttu-id="324a8-212">El algoritmo de red neuronal de [!INCLUDE[msCoName](../../includes/msconame-md.md)] admite varios parámetros que afectan al comportamiento, al rendimiento y a la precisión del modelo de minería de datos resultante.</span><span class="sxs-lookup"><span data-stu-id="324a8-212">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm supports several parameters that affect the behavior, performance, and accuracy of the resulting mining model.</span></span> <span data-ttu-id="324a8-213">También puede modificar la forma en la que el modelo procesa los datos; para ello, puede establecer marcas de modelado en las columnas o marcas de distribución que especifiquen cómo se deben procesar los valores dentro de la columna.</span><span class="sxs-lookup"><span data-stu-id="324a8-213">You can also modify the way that the model processes data by setting modeling flags on columns, or by setting distribution flags to specify how values within the column are handled.</span></span>  
  
### <a name="setting-algorithm-parameters"></a><span data-ttu-id="324a8-214">Establecer parámetros del algoritmo</span><span class="sxs-lookup"><span data-stu-id="324a8-214">Setting Algorithm Parameters</span></span>  
 <span data-ttu-id="324a8-215">En la tabla siguiente, se describen los parámetros que se pueden usar con el algoritmo de red neuronal de Microsoft.</span><span class="sxs-lookup"><span data-stu-id="324a8-215">The following table describes the parameters that can be used with the Microsoft Neural Network algorithm.</span></span>  
  
 <span data-ttu-id="324a8-216">HIDDEN_NODE_RATIO</span><span class="sxs-lookup"><span data-stu-id="324a8-216">HIDDEN_NODE_RATIO</span></span>  
 <span data-ttu-id="324a8-217">Especifica la proporción de neuronas ocultas por neuronas de entrada y de salida.</span><span class="sxs-lookup"><span data-stu-id="324a8-217">Specifies the ratio of hidden neurons to input and output neurons.</span></span> <span data-ttu-id="324a8-218">La siguiente fórmula determina el número inicial de neuronas de la capa oculta:</span><span class="sxs-lookup"><span data-stu-id="324a8-218">The following formula determines the initial number of neurons in the hidden layer:</span></span>  
  
 <span data-ttu-id="324a8-219">HIDDEN_NODE_RATIO \* SQRT(Total de neuronas de entrada \* Total de neuronas de salida)</span><span class="sxs-lookup"><span data-stu-id="324a8-219">HIDDEN_NODE_RATIO \* SQRT(Total input neurons \* Total output neurons)</span></span>  
  
 <span data-ttu-id="324a8-220">El valor predeterminado es 4,0.</span><span class="sxs-lookup"><span data-stu-id="324a8-220">The default value is 4.0.</span></span>  
  
 <span data-ttu-id="324a8-221">HOLDOUT_PERCENTAGE</span><span class="sxs-lookup"><span data-stu-id="324a8-221">HOLDOUT_PERCENTAGE</span></span>  
 <span data-ttu-id="324a8-222">Especifica el porcentaje de casos de los datos de entrenamiento que se han usado para calcular el error de datos de exclusión, que se usa como parte de los criterios de detención durante el entrenamiento del modelo de minería de datos.</span><span class="sxs-lookup"><span data-stu-id="324a8-222">Specifies the percentage of cases within the training data used to calculate the holdout error, which is used as part of the stopping criteria while training the mining model.</span></span>  
  
 <span data-ttu-id="324a8-223">El valor predeterminado es 30.</span><span class="sxs-lookup"><span data-stu-id="324a8-223">The default value is 30.</span></span>  
  
 <span data-ttu-id="324a8-224">HOLDOUT_SEED</span><span class="sxs-lookup"><span data-stu-id="324a8-224">HOLDOUT_SEED</span></span>  
 <span data-ttu-id="324a8-225">Especifica un número que se usa para inicializar el generador pseudoaleatorio cuando el algoritmo determina aleatoriamente los datos de exclusión.</span><span class="sxs-lookup"><span data-stu-id="324a8-225">Specifies a number that is used to seed the pseudo-random generator when the algorithm randomly determines the holdout data.</span></span> <span data-ttu-id="324a8-226">Si este parámetro se establece en 0, el algoritmo genera la inicialización basada en el nombre del modelo de minería de datos para garantizar que el contenido del modelo permanece intacto al volver a realizar el proceso.</span><span class="sxs-lookup"><span data-stu-id="324a8-226">If this parameter is set to 0, the algorithm generates the seed based on the name of the mining model, to guarantee that the model content remains the same during reprocessing.</span></span>  
  
 <span data-ttu-id="324a8-227">El valor predeterminado es 0.</span><span class="sxs-lookup"><span data-stu-id="324a8-227">The default value is 0.</span></span>  
  
 <span data-ttu-id="324a8-228">MAXIMUM_INPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="324a8-228">MAXIMUM_INPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="324a8-229">Determina el número máximo de atributos de entrada que se pueden proporcionar al algoritmo antes de emplear la selección de características.</span><span class="sxs-lookup"><span data-stu-id="324a8-229">Determines the maximum number of input attributes that can be supplied to the algorithm before feature selection is employed.</span></span> <span data-ttu-id="324a8-230">La función de selección de atributos de entrada se deshabilita cuando este valor se establece en 0.</span><span class="sxs-lookup"><span data-stu-id="324a8-230">Setting this value to 0 disables feature selection for input attributes.</span></span>  
  
 <span data-ttu-id="324a8-231">El valor predeterminado es 255.</span><span class="sxs-lookup"><span data-stu-id="324a8-231">The default value is 255.</span></span>  
  
 <span data-ttu-id="324a8-232">MAXIMUM_OUTPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="324a8-232">MAXIMUM_OUTPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="324a8-233">Determina el número máximo de atributos de salida que se pueden proporcionar al algoritmo antes de emplear la selección de características.</span><span class="sxs-lookup"><span data-stu-id="324a8-233">Determines the maximum number of output attributes that can be supplied to the algorithm before feature selection is employed.</span></span> <span data-ttu-id="324a8-234">La característica de selección de atributos de salida se deshabilita cuando este valor se establece en 0.</span><span class="sxs-lookup"><span data-stu-id="324a8-234">Setting this value to 0 disables feature selection for output attributes.</span></span>  
  
 <span data-ttu-id="324a8-235">El valor predeterminado es 255.</span><span class="sxs-lookup"><span data-stu-id="324a8-235">The default value is 255.</span></span>  
  
 <span data-ttu-id="324a8-236">MAXIMUM_STATES</span><span class="sxs-lookup"><span data-stu-id="324a8-236">MAXIMUM_STATES</span></span>  
 <span data-ttu-id="324a8-237">Especifica el número máximo de estados discretos por atributo que admite el algoritmo.</span><span class="sxs-lookup"><span data-stu-id="324a8-237">Specifies the maximum number of discrete states per attribute that is supported by the algorithm.</span></span> <span data-ttu-id="324a8-238">Si en número de estados de un atributo específico es mayor que el número especificado para este parámetro, el algoritmo utiliza los estados más frecuentes de este atributo y trata al resto como estados que faltan.</span><span class="sxs-lookup"><span data-stu-id="324a8-238">If the number of states for a specific attribute is greater than the number that is specified for this parameter, the algorithm uses the most popular states for that attribute and treats the remaining states as missing.</span></span>  
  
 <span data-ttu-id="324a8-239">El valor predeterminado es 100.</span><span class="sxs-lookup"><span data-stu-id="324a8-239">The default value is 100.</span></span>  
  
 <span data-ttu-id="324a8-240">SAMPLE_SIZE</span><span class="sxs-lookup"><span data-stu-id="324a8-240">SAMPLE_SIZE</span></span>  
 <span data-ttu-id="324a8-241">Especifica el número de casos que se van a usar para realizar el entrenamiento del modelo.</span><span class="sxs-lookup"><span data-stu-id="324a8-241">Specifies the number of cases to be used to train the model.</span></span> <span data-ttu-id="324a8-242">El algoritmo utiliza el valor menor entre este número o el porcentaje del total de escenarios que no están incluidos en los datos de exclusión, según se especifica en el parámetro HOLDOUT_PERCENTAGE.</span><span class="sxs-lookup"><span data-stu-id="324a8-242">The algorithm uses either this number or the percentage of total of cases not included in the holdout data as specified by the HOLDOUT_PERCENTAGE parameter, whichever value is smaller.</span></span>  
  
 <span data-ttu-id="324a8-243">En otras palabras, si HOLDOUT_PERCENTAGE se establece en 30, el algoritmo utilizará el valor de este parámetro o un valor igual al 70 por ciento del número total de casos, según cuál sea menor.</span><span class="sxs-lookup"><span data-stu-id="324a8-243">In other words, if HOLDOUT_PERCENTAGE is set to 30, the algorithm will use either the value of this parameter, or a value equal to 70 percent of the total number of cases, whichever is smaller.</span></span>  
  
 <span data-ttu-id="324a8-244">El valor predeterminado es 10000.</span><span class="sxs-lookup"><span data-stu-id="324a8-244">The default value is 10000.</span></span>  
  
### <a name="modeling-flags"></a><span data-ttu-id="324a8-245">Marcas de modelado</span><span class="sxs-lookup"><span data-stu-id="324a8-245">Modeling Flags</span></span>  
 <span data-ttu-id="324a8-246">El algoritmo de red neuronal de [!INCLUDE[msCoName](../../includes/msconame-md.md)] admite las siguientes marcas de modelado.</span><span class="sxs-lookup"><span data-stu-id="324a8-246">The following modeling flags are supported for use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span>  
  
 <span data-ttu-id="324a8-247">NOT NULL</span><span class="sxs-lookup"><span data-stu-id="324a8-247">NOT NULL</span></span>  
 <span data-ttu-id="324a8-248">Indica que la columna no puede contener un valor NULL.</span><span class="sxs-lookup"><span data-stu-id="324a8-248">Indicates that the column cannot contain a null.</span></span> <span data-ttu-id="324a8-249">Se producirá un error si Analysis Services encuentra un valor NULL durante el entrenamiento del modelo.</span><span class="sxs-lookup"><span data-stu-id="324a8-249">An error will result if Analysis Services encounters a null during model training.</span></span>  
  
 <span data-ttu-id="324a8-250">Se aplica a las columnas de la estructura de minería de datos.</span><span class="sxs-lookup"><span data-stu-id="324a8-250">Applies to mining structure columns.</span></span>  
  
 <span data-ttu-id="324a8-251">MODEL_EXISTENCE_ONLY</span><span class="sxs-lookup"><span data-stu-id="324a8-251">MODEL_EXISTENCE_ONLY</span></span>  
 <span data-ttu-id="324a8-252">Indica que el modelo solo debe considerar si existe un valor para el atributo o si falta un valor.</span><span class="sxs-lookup"><span data-stu-id="324a8-252">Indicates that the model should only consider whether a value exists for the attribute or if a value is missing.</span></span> <span data-ttu-id="324a8-253">No importa el valor exacto.</span><span class="sxs-lookup"><span data-stu-id="324a8-253">The exact value does not matter.</span></span>  
  
 <span data-ttu-id="324a8-254">Se aplica a las columnas del modelo de minería de datos.</span><span class="sxs-lookup"><span data-stu-id="324a8-254">Applies to mining model columns.</span></span>  
  
### <a name="distribution-flags"></a><span data-ttu-id="324a8-255">Marcas de distribución</span><span class="sxs-lookup"><span data-stu-id="324a8-255">Distribution Flags</span></span>  
 <span data-ttu-id="324a8-256">El algoritmo de red neuronal de [!INCLUDE[msCoName](../../includes/msconame-md.md)] admite las siguientes marcas de distribución.</span><span class="sxs-lookup"><span data-stu-id="324a8-256">The following distribution flags are supported for use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span> <span data-ttu-id="324a8-257">Las marcas solo se usan como sugerencias para el modelo; si el algoritmo detecta una distribución diferente, usará la distribución encontrada, no la proporcionada en la sugerencia.</span><span class="sxs-lookup"><span data-stu-id="324a8-257">The flags are used as hints to the model only; if the algorithm detects a different distribution it will use the found distribution, not the distribution provided in the hint.</span></span>  
  
 <span data-ttu-id="324a8-258">Normal</span><span class="sxs-lookup"><span data-stu-id="324a8-258">Normal</span></span>  
 <span data-ttu-id="324a8-259">Indica que los valores de la columna se deben tratar como si representasen la distribución normal o gaussiana.</span><span class="sxs-lookup"><span data-stu-id="324a8-259">Indicates that values within the column should be treated as though they represent the normal, or Gaussian, distribution.</span></span>  
  
 <span data-ttu-id="324a8-260">Uniforme</span><span class="sxs-lookup"><span data-stu-id="324a8-260">Uniform</span></span>  
 <span data-ttu-id="324a8-261">Indica que los valores de la columna se deben tratar como si estuviesen distribuidos uniformemente; es decir, la probabilidad de cualquier valor es más o menos la misma y depende del número total de valores.</span><span class="sxs-lookup"><span data-stu-id="324a8-261">Indicates that values within the column should be treated as though they are distributed uniformly; that is, the probability of any value is roughly equal, and is a function of the total number of values.</span></span>  
  
 <span data-ttu-id="324a8-262">Logarítmica normal</span><span class="sxs-lookup"><span data-stu-id="324a8-262">Log Normal</span></span>  
 <span data-ttu-id="324a8-263">Indica que los valores de la columna tienen que tratarse como si estuviesen distribuidos según la curva *logarítmica normal* , lo que significa que el logaritmo de los valores se distribuye normalmente.</span><span class="sxs-lookup"><span data-stu-id="324a8-263">Indicates that values within the column should be treated as though distributed according to the *log normal* curve, which means that the logarithm of the values is distributed normally.</span></span>  
  
## <a name="requirements"></a><span data-ttu-id="324a8-264">Requisitos</span><span class="sxs-lookup"><span data-stu-id="324a8-264">Requirements</span></span>  
 <span data-ttu-id="324a8-265">Un modelo de red neuronal debe contener por lo menos una columna de entrada y una columna de salida.</span><span class="sxs-lookup"><span data-stu-id="324a8-265">A neural network model must contain at least one input column and one output column.</span></span>  
  
### <a name="input-and-predictable-columns"></a><span data-ttu-id="324a8-266">Columnas de entrada y de predicción</span><span class="sxs-lookup"><span data-stu-id="324a8-266">Input and Predictable Columns</span></span>  
 <span data-ttu-id="324a8-267">El algoritmo de red neuronal de [!INCLUDE[msCoName](../../includes/msconame-md.md)] admite las columnas de entrada y de predicción específicas que se enumeran en la tabla siguiente.</span><span class="sxs-lookup"><span data-stu-id="324a8-267">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm supports the specific input columns and predictable columns that are listed in the following table.</span></span>  
  
|<span data-ttu-id="324a8-268">Columna</span><span class="sxs-lookup"><span data-stu-id="324a8-268">Column</span></span>|<span data-ttu-id="324a8-269">Tipos de contenido</span><span class="sxs-lookup"><span data-stu-id="324a8-269">Content types</span></span>|  
|------------|-------------------|  
|<span data-ttu-id="324a8-270">Atributo de entrada</span><span class="sxs-lookup"><span data-stu-id="324a8-270">Input attribute</span></span>|<span data-ttu-id="324a8-271">Continuous, Cyclical, Discrete, Discretized, Key, Table y Ordered</span><span class="sxs-lookup"><span data-stu-id="324a8-271">Continuous, Cyclical, Discrete, Discretized, Key, Table, and Ordered</span></span>|  
|<span data-ttu-id="324a8-272">Atributo de predicción</span><span class="sxs-lookup"><span data-stu-id="324a8-272">Predictable attribute</span></span>|<span data-ttu-id="324a8-273">Continuous, Cyclical, Discrete, Discretized y Ordered</span><span class="sxs-lookup"><span data-stu-id="324a8-273">Continuous, Cyclical, Discrete, Discretized, and Ordered</span></span>|  
  
> [!NOTE]  
>  <span data-ttu-id="324a8-274">Se admiten los tipos de contenido Cyclical y Ordered, pero el algoritmo los trata como valores discretos y no realiza un procesamiento especial.</span><span class="sxs-lookup"><span data-stu-id="324a8-274">Cyclical and Ordered content types are supported, but the algorithm treats them as discrete values and does not perform special processing.</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="324a8-275">Consulte también</span><span class="sxs-lookup"><span data-stu-id="324a8-275">See Also</span></span>  
 <span data-ttu-id="324a8-276">[Algoritmo de red neuronal de Microsoft](microsoft-neural-network-algorithm.md) </span><span class="sxs-lookup"><span data-stu-id="324a8-276">[Microsoft Neural Network Algorithm](microsoft-neural-network-algorithm.md) </span></span>  
 <span data-ttu-id="324a8-277">[Contenido del modelo de minería de datos para los modelos de red neuronal &#40;Analysis Services-minería de datos&#41;](mining-model-content-for-neural-network-models-analysis-services-data-mining.md) </span><span class="sxs-lookup"><span data-stu-id="324a8-277">[Mining Model Content for Neural Network Models &#40;Analysis Services - Data Mining&#41;](mining-model-content-for-neural-network-models-analysis-services-data-mining.md) </span></span>  
 [<span data-ttu-id="324a8-278">Ejemplos de consultas de modelos de red neuronal</span><span class="sxs-lookup"><span data-stu-id="324a8-278">Neural Network Model Query Examples</span></span>](neural-network-model-query-examples.md)  
  
  
