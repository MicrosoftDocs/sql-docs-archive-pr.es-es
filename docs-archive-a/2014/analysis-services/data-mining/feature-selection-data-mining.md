---
title: Selección de características (minería de datos) | Microsoft Docs
ms.custom: ''
ms.date: 03/06/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- mining models [Analysis Services], feature selections
- attributes [data mining]
- feature selection algorithms [Analysis Services]
- data mining [Analysis Services], feature selections
- neural network algorithms [Analysis Services]
- naive bayes algorithms [Analysis Services]
- decision tree algorithms [Analysis Services]
- datasets [Analysis Services]
- clustering algorithms [Analysis Services]
- coding [Data Mining]
ms.assetid: b044e785-4875-45ab-8ae4-cd3b4e3033bb
author: minewiskan
ms.author: owend
ms.openlocfilehash: ef5ee56636e7710074a893aed733905e1bf983bd
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: es-ES
ms.lasthandoff: 08/04/2020
ms.locfileid: "87674448"
---
# <a name="feature-selection-data-mining"></a><span data-ttu-id="3a52c-102">Selección de características (minería de datos)</span><span class="sxs-lookup"><span data-stu-id="3a52c-102">Feature Selection (Data Mining)</span></span>
  <span data-ttu-id="3a52c-103">La *selección de características* es un término que se usa habitualmente en la minería de datos para describir las herramientas y técnicas disponibles para reducir las entradas a un tamaño manejable para el procesamiento y el análisis.</span><span class="sxs-lookup"><span data-stu-id="3a52c-103">*Feature selection* is a term commonly used in data mining to describe the tools and techniques available for reducing inputs to a manageable size for processing and analysis.</span></span> <span data-ttu-id="3a52c-104">La selección de características implica no solo la *reducción de cardinalidad*, lo que significa imponer un límite arbitrario o predefinido en el número de atributos que se pueden tener en cuenta al crear un modelo, pero también la elección de atributos, lo que significa que el analista o la herramienta de modelado seleccionan o descartan atributos en función de su utilidad para el análisis.</span><span class="sxs-lookup"><span data-stu-id="3a52c-104">Feature selection implies not only *cardinality reduction*, which means imposing an arbitrary or predefined cutoff on the number of attributes that can be considered when building a model, but also the choice of attributes, meaning that either the analyst or the modeling tool actively selects or discards attributes based on their usefulness for analysis.</span></span>  
  
 <span data-ttu-id="3a52c-105">La capacidad de aplicar la selección de características es esencial para un análisis eficiente, ya que los conjuntos de datos suelen contener mucha más información de la necesaria para la generación del modelo.</span><span class="sxs-lookup"><span data-stu-id="3a52c-105">The ability to apply feature selection is critical for effective analysis, because datasets frequently contain far more information than is needed to build the model.</span></span> <span data-ttu-id="3a52c-106">Por ejemplo, un conjunto de datos puede contener 500 columnas que describen las características de los clientes, pero si los datos de algunas de ellas están muy dispersos, no obtendrá muchas ventajas al agregarlas al modelo.</span><span class="sxs-lookup"><span data-stu-id="3a52c-106">For example, a dataset might contain 500 columns that describe the characteristics of customers, but if the data in some of the columns is very sparse you would gain very little benefit from adding them to the model.</span></span> <span data-ttu-id="3a52c-107">Si mantiene las columnas innecesarias durante la generación del modelo, se necesitarán más CPU y memoria durante el proceso de entrenamiento, así como más espacio de almacenamiento para el modelo completado.</span><span class="sxs-lookup"><span data-stu-id="3a52c-107">If you keep the unneeded columns while building the model, more CPU and memory are required during the training process, and more storage space is required for the completed model.</span></span>  
  
 <span data-ttu-id="3a52c-108">Aunque los recursos no sean un problema, normalmente se deben quitar las columnas innecesarias porque pueden degradar la calidad de los patrones detectados por las razones siguientes:</span><span class="sxs-lookup"><span data-stu-id="3a52c-108">Even if resources are not an issue, you typically want to remove unneeded columns because they might degrade the quality of discovered patterns, for the following reasons:</span></span>  
  
-   <span data-ttu-id="3a52c-109">Algunas columnas son ruidosas o redundantes.</span><span class="sxs-lookup"><span data-stu-id="3a52c-109">Some columns are noisy or redundant.</span></span> <span data-ttu-id="3a52c-110">Este ruido dificulta la detección de patrones significativos a partir de los datos.</span><span class="sxs-lookup"><span data-stu-id="3a52c-110">This noise makes it more difficult to discover meaningful patterns from the data;</span></span>  
  
-   <span data-ttu-id="3a52c-111">Para detectar patrones de calidad, la mayoría de los algoritmos de minería de datos requieren un conjunto de datos de entrenamiento mucho más grande en un conjunto de datos multidimensional.</span><span class="sxs-lookup"><span data-stu-id="3a52c-111">To discover quality patterns, most data mining algorithms require much larger training data set on high-dimensional data set.</span></span> <span data-ttu-id="3a52c-112">Sin embargo, en algunas aplicaciones de minería de datos se dispone de muy pocos datos de entrenamiento.</span><span class="sxs-lookup"><span data-stu-id="3a52c-112">But the training data is very small in some data mining applications.</span></span>  
  
 <span data-ttu-id="3a52c-113">Si solo 50 de las 500 columnas del origen de datos tienen información útil para la generación de un modelo, puede dejar fuera del modelo las que no son útiles o usar técnicas de selección de características para detectar automáticamente las mejores características y excluir los valores estadísticamente no significativos.</span><span class="sxs-lookup"><span data-stu-id="3a52c-113">If only 50 of the 500 columns in the data source have information that is useful in building a model, you could just leave them out of the model, or you could use feature selection techniques to automatically discover the best features and to exclude values that are statistically insignificant.</span></span> <span data-ttu-id="3a52c-114">La selección de características ayuda a resolver el problema de tener demasiados datos de escaso valor o muy pocos datos de mucho valor.</span><span class="sxs-lookup"><span data-stu-id="3a52c-114">Feature selection helps solve the twin problems of having too much data that is of little value, or having too little data that is of high value.</span></span>  
  
## <a name="feature-selection-in-analysis-services-data-mining"></a><span data-ttu-id="3a52c-115">Selección de características en minería de datos de Analysis Services</span><span class="sxs-lookup"><span data-stu-id="3a52c-115">Feature Selection in Analysis Services Data Mining</span></span>  
 <span data-ttu-id="3a52c-116">Normalmente, la selección de características se realiza automáticamente en [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)], y cada algoritmo tiene un conjunto de técnicas predeterminadas para aplicar de forma inteligente la reducción de características.</span><span class="sxs-lookup"><span data-stu-id="3a52c-116">Usually, feature selection is performed automatically in [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)], and each algorithm has a set of default techniques for intelligently applying feature reduction.</span></span> <span data-ttu-id="3a52c-117">La selección de características siempre se realiza antes del entrenamiento del modelo y permite elegir automáticamente en un conjunto de datos los atributos que con toda probabilidad se usarán en el mismo.</span><span class="sxs-lookup"><span data-stu-id="3a52c-117">Feature selection is always performed before the model is trained, to automatically choose the attributes in a dataset that are most likely to be used in the model.</span></span> <span data-ttu-id="3a52c-118">Sin embargo, también puede establecer parámetros manualmente para influir en el comportamiento de la selección de características.</span><span class="sxs-lookup"><span data-stu-id="3a52c-118">However, you can also manually set parameters to influence feature selection behavior.</span></span>  
  
 <span data-ttu-id="3a52c-119">En general, la selección de características funciona calculando una puntuación para cada atributo y seleccionando a continuación solo los atributos que han obtenido las mejores puntuaciones.</span><span class="sxs-lookup"><span data-stu-id="3a52c-119">In general, feature selection works by calculating a score for each attribute, and then selecting only the attributes that have the best scores.</span></span> <span data-ttu-id="3a52c-120">También puede ajustar el umbral para las puntuaciones más altas.</span><span class="sxs-lookup"><span data-stu-id="3a52c-120">You can also adjust the threshold for the top scores.</span></span> [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] <span data-ttu-id="3a52c-121">proporciona varios métodos para calcular estas puntuaciones, y el método exacto que se aplica en un modelo depende de estos factores:</span><span class="sxs-lookup"><span data-stu-id="3a52c-121">provides multiple methods for calculating these scores, and the exact method that is applied in any model depends on these factors:</span></span>  
  
-   <span data-ttu-id="3a52c-122">El algoritmo usado en el modelo</span><span class="sxs-lookup"><span data-stu-id="3a52c-122">The algorithm used in your model</span></span>  
  
-   <span data-ttu-id="3a52c-123">El tipo de datos del atributo</span><span class="sxs-lookup"><span data-stu-id="3a52c-123">The data type of the attribute</span></span>  
  
-   <span data-ttu-id="3a52c-124">Otros parámetros que se hayan podido establecer en el modelo</span><span class="sxs-lookup"><span data-stu-id="3a52c-124">Any parameters that you may have set on your model</span></span>  
  
 <span data-ttu-id="3a52c-125">La selección de características se aplica a las entradas, a los atributos de predicción o a los estados de una columna.</span><span class="sxs-lookup"><span data-stu-id="3a52c-125">Feature selection is applied to inputs, predictable attributes, or to states in a column.</span></span> <span data-ttu-id="3a52c-126">Una vez completada la puntuación para la selección de características, solo se incluyen en el proceso de generación del modelo y se pueden usar en la predicción los atributos y los estados que selecciona el algoritmo.</span><span class="sxs-lookup"><span data-stu-id="3a52c-126">When scoring for feature selection is complete, only the attributes and states that the algorithm selects are included in the model-building process and can be used for prediction.</span></span> <span data-ttu-id="3a52c-127">Si elige un atributo de predicción que no alcanza el umbral para la selección de características, dicho atributo se puede usar en la predicción, pero las predicciones se basarán únicamente en las estadísticas globales que existen en el modelo.</span><span class="sxs-lookup"><span data-stu-id="3a52c-127">If you choose a predictable attribute that does not meet the threshold for feature selection the attribute can still be used for prediction, but the predictions will be based solely on the global statistics that exist in the model.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="3a52c-128">La selección de características afecta solo a las columnas que se usan en el modelo y no tiene ningún efecto en el almacenamiento de la estructura de minería de datos.</span><span class="sxs-lookup"><span data-stu-id="3a52c-128">Feature selection affects only the columns that are used in the model, and has no effect on storage of the mining structure.</span></span> <span data-ttu-id="3a52c-129">Las columnas que se dejan fuera del modelo de minería de datos siguen estando disponibles en la estructura, y los datos de las columnas de la estructura de minería de datos se almacenan en caché.</span><span class="sxs-lookup"><span data-stu-id="3a52c-129">The columns that you leave out of the mining model are still available in the structure, and data in the mining structure columns will be cached.</span></span>  
  
### <a name="definition-of-feature-selection-methods"></a><span data-ttu-id="3a52c-130">Definición de los métodos de selección de características</span><span class="sxs-lookup"><span data-stu-id="3a52c-130">Definition of Feature Selection Methods</span></span>  
 <span data-ttu-id="3a52c-131">Hay muchas maneras de implementar la selección de características, dependiendo del tipo de datos con los que se esté trabajando y del algoritmo que se elija para el análisis.</span><span class="sxs-lookup"><span data-stu-id="3a52c-131">There are many ways to implement feature selection, depending on the type of data that you are working with and the algorithm that you choose for analysis.</span></span> <span data-ttu-id="3a52c-132">SQL Server Analysis Services proporciona varios métodos conocidos y consolidados para puntuar los atributos.</span><span class="sxs-lookup"><span data-stu-id="3a52c-132">SQL Server Analysis Services provides several popular and well-established methods for scoring attributes.</span></span> <span data-ttu-id="3a52c-133">El método que se aplica en los algoritmos o conjuntos de datos depende de los tipos de datos, así como del uso de las columnas.</span><span class="sxs-lookup"><span data-stu-id="3a52c-133">The method that is applied in any algorithm or data set depends on the data types, and the column usage.</span></span>  
  
 <span data-ttu-id="3a52c-134">La puntuación *interestingness* (medida del interés) se usa para clasificar y ordenar los atributos de las columnas que contienen datos numéricos continuos no binarios.</span><span class="sxs-lookup"><span data-stu-id="3a52c-134">The *interestingness* score is used to rank and sort attributes in columns that contain nonbinary continuous numeric data.</span></span>  
  
 <span data-ttu-id="3a52c-135">La*entropía de Shannon* y dos puntuaciones *bayesianas* están disponibles para las columnas que contengan datos discretos y discretizados.</span><span class="sxs-lookup"><span data-stu-id="3a52c-135">*Shannon's entropy* and two *Bayesian* scores are available for columns that contain discrete and discretized data.</span></span> <span data-ttu-id="3a52c-136">Sin embargo, si el modelo contiene columnas continuas, se usará la puntuación de grado de interés para evaluar todas las columnas de entrada, con objeto de asegurarse de que son coherentes.</span><span class="sxs-lookup"><span data-stu-id="3a52c-136">However, if the model contains any continuous columns, the interestingness score will be used to assess all input columns, to ensure consistency.</span></span>  
  
 <span data-ttu-id="3a52c-137">En la sección siguiente se describe cada uno de los métodos de selección de características.</span><span class="sxs-lookup"><span data-stu-id="3a52c-137">The following section describes each method of feature selection.</span></span>  
  
#### <a name="interestingness-score"></a><span data-ttu-id="3a52c-138">Puntuación interestingness</span><span class="sxs-lookup"><span data-stu-id="3a52c-138">Interestingness score</span></span>  
 <span data-ttu-id="3a52c-139">Una característica es interesante si ofrece información útil.</span><span class="sxs-lookup"><span data-stu-id="3a52c-139">A feature is interesting if it tells you some useful piece of information.</span></span> <span data-ttu-id="3a52c-140">Dado que la definición de lo que resulta útil varía en función del escenario, el sector de minería de datos ha desarrollado varias maneras de medir la *interés*.</span><span class="sxs-lookup"><span data-stu-id="3a52c-140">Because the definition of what is useful varies depending on the scenario, the data mining industry has developed various ways to measure *interestingness*.</span></span> <span data-ttu-id="3a52c-141">Por ejemplo, la *novedad* podría ser interesante en la detección de valores atípicos, pero la capacidad de discriminar entre elementos estrechamente relacionados, o *peso discriminador*, podría ser más interesante para la clasificación.</span><span class="sxs-lookup"><span data-stu-id="3a52c-141">For example, *novelty* might be interesting in outlier detection, but the ability to discriminate between closely related items, or *discriminating weight*, might be more interesting for classification.</span></span>  
  
 <span data-ttu-id="3a52c-142">La medida de interés que se usa en SQL Server Analysis Services se basa en la *entropía*, lo que significa que los atributos con distribuciones aleatorias tienen una entropía más alta y una ganancia inferior de la información. por lo tanto, estos atributos son menos interesantes.</span><span class="sxs-lookup"><span data-stu-id="3a52c-142">The measure of interestingness that is used in SQL Server Analysis Services is *entropy-based*, meaning that attributes with random distributions have higher entropy and lower information gain; therefore, such attributes are less interesting.</span></span> <span data-ttu-id="3a52c-143">La entropía para cualquier atributo se compara con la entropía de todos los demás atributos de la manera siguiente:</span><span class="sxs-lookup"><span data-stu-id="3a52c-143">The entropy for any particular attribute is compared to the entropy of all other attributes, as follows:</span></span>  
  
 <span data-ttu-id="3a52c-144">Interestingness(Atributo) = - (m - Entropy(Atributo)) \* (m - Entropy(Atributo))</span><span class="sxs-lookup"><span data-stu-id="3a52c-144">Interestingness(Attribute) = - (m - Entropy(Attribute)) \* (m - Entropy(Attribute))</span></span>  
  
 <span data-ttu-id="3a52c-145">La entropía central (o m) es la entropía de todo el conjunto de características.</span><span class="sxs-lookup"><span data-stu-id="3a52c-145">Central entropy, or m, means the entropy of the entire feature set.</span></span> <span data-ttu-id="3a52c-146">Al restar la entropía del atributo de destino de la entropía central, se puede evaluar cuánta información proporciona el atributo.</span><span class="sxs-lookup"><span data-stu-id="3a52c-146">By subtracting the entropy of the target attribute from the central entropy, you can assess how much information the attribute provides.</span></span>  
  
 <span data-ttu-id="3a52c-147">Esta puntuación se utiliza de forma predeterminada cada vez que la columna contiene datos numéricos continuos no binarios.</span><span class="sxs-lookup"><span data-stu-id="3a52c-147">This score is used by default whenever the column contains nonbinary continuous numeric data.</span></span>  
  
#### <a name="shannons-entropy"></a><span data-ttu-id="3a52c-148">Entropía de Shannon</span><span class="sxs-lookup"><span data-stu-id="3a52c-148">Shannon's Entropy</span></span>  
 <span data-ttu-id="3a52c-149">La entropía de Shannon mide la incertidumbre de una variable aleatoria para un determinado resultado.</span><span class="sxs-lookup"><span data-stu-id="3a52c-149">Shannon's entropy measures the uncertainty of a random variable for a particular outcome.</span></span> <span data-ttu-id="3a52c-150">Por ejemplo, la entropía de lanzar una moneda al aire para decidir algo a cara o cruz se puede representar como una función de la probabilidad de que salga cara.</span><span class="sxs-lookup"><span data-stu-id="3a52c-150">For example, the entropy of a coin toss can be represented as a function of the probability of it coming up heads.</span></span>  
  
 <span data-ttu-id="3a52c-151">Analysis Services usa la fórmula siguiente para calcular la entropía de Shannon:</span><span class="sxs-lookup"><span data-stu-id="3a52c-151">Analysis Services uses the following formula to calculate Shannon's entropy:</span></span>  
  
 <span data-ttu-id="3a52c-152">H(X) = -∑ P(xi) log(P(xi))</span><span class="sxs-lookup"><span data-stu-id="3a52c-152">H(X) = -∑ P(xi) log(P(xi))</span></span>  
  
 <span data-ttu-id="3a52c-153">Este método de puntuación está disponible para los atributos discretos y de datos discretos.</span><span class="sxs-lookup"><span data-stu-id="3a52c-153">This scoring method is available for discrete and discretized attributes.</span></span>  
  
#### <a name="bayesian-with-k2-prior"></a><span data-ttu-id="3a52c-154">Bayesiano con prioridad K2</span><span class="sxs-lookup"><span data-stu-id="3a52c-154">Bayesian with K2 Prior</span></span>  
 <span data-ttu-id="3a52c-155">Analysis Services proporciona dos puntuaciones de selección de características basadas en las redes bayesianas.</span><span class="sxs-lookup"><span data-stu-id="3a52c-155">Analysis Services provides two feature selection scores that are based on Bayesian networks.</span></span> <span data-ttu-id="3a52c-156">Una red bayesiana es un gráfico *dirigido* o *acíclico* de estados y de transiciones entre ellos; esto significa que algunos estados siempre son anteriores al estado actual y otros son posteriores, y que el gráfico no se repite ni realiza bucles.</span><span class="sxs-lookup"><span data-stu-id="3a52c-156">A Bayesian network is a *directed* or *acyclic* graph of states and transitions between states, meaning that some states are always prior to the current state, some states are posterior, and the graph does not repeat or loop.</span></span> <span data-ttu-id="3a52c-157">Por definición, las redes bayesianas permiten el uso del conocimiento previo.</span><span class="sxs-lookup"><span data-stu-id="3a52c-157">By definition, Bayesian networks allow the use of prior knowledge.</span></span> <span data-ttu-id="3a52c-158">Sin embargo, la pregunta sobre qué estados anteriores se deben utilizar para calcular las probabilidades de los estados posteriores es importante para la precisión, el rendimiento y el diseño del algoritmo.</span><span class="sxs-lookup"><span data-stu-id="3a52c-158">However, the question of which prior states to use in calculating probabilities of later states is important for algorithm design, performance, and accuracy.</span></span>  
  
 <span data-ttu-id="3a52c-159">Cooper y Herskovits desarrollaron el algoritmo K2 para el aprendizaje a partir de una red bayesiana y este algoritmo se utiliza a menudo en la minería de datos.</span><span class="sxs-lookup"><span data-stu-id="3a52c-159">The K2 algorithm for learning from a Bayesian network was developed by Cooper and Herskovits and is often used in data mining.</span></span> <span data-ttu-id="3a52c-160">Es escalable y puede analizar varias variables, pero requiere la ordenación de las variables utilizadas como entrada.</span><span class="sxs-lookup"><span data-stu-id="3a52c-160">It is scalable and can analyze multiple variables, but requires ordering on variables used as input.</span></span> <span data-ttu-id="3a52c-161">Para obtener más información, vea el documento [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885) por Chickering, Geiger y Heckerman.</span><span class="sxs-lookup"><span data-stu-id="3a52c-161">For more information, see [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885) by Chickering, Geiger, and Heckerman.</span></span>  
  
 <span data-ttu-id="3a52c-162">Este método de puntuación está disponible para los atributos discretos y de datos discretos.</span><span class="sxs-lookup"><span data-stu-id="3a52c-162">This scoring method is available for discrete and discretized attributes.</span></span>  
  
#### <a name="bayesian-dirichlet-equivalent-with-uniform-prior"></a><span data-ttu-id="3a52c-163">Equivalente Dirichlet bayesiano con prioridad uniforme</span><span class="sxs-lookup"><span data-stu-id="3a52c-163">Bayesian Dirichlet Equivalent with Uniform Prior</span></span>  
 <span data-ttu-id="3a52c-164">La puntuación Equivalente Dirichlet bayesiano (BDE) también utiliza el análisis bayesiano para evaluar una red dado un conjunto de datos.</span><span class="sxs-lookup"><span data-stu-id="3a52c-164">The Bayesian Dirichlet Equivalent (BDE) score also uses Bayesian analysis to evaluate a network given a dataset.</span></span> <span data-ttu-id="3a52c-165">El método de puntuación BDE fue desarrollado por Heckerman y está basado en la métrica BD desarrollada por Cooper y Herskovits.</span><span class="sxs-lookup"><span data-stu-id="3a52c-165">The BDE scoring method was developed by Heckerman and is based on the BD metric developed by Cooper and Herskovits.</span></span> <span data-ttu-id="3a52c-166">La distribución Dirichlet es una distribución multinomial que describe la probabilidad condicional de cada variable de la red y dispone de muchas propiedades que son útiles para el aprendizaje.</span><span class="sxs-lookup"><span data-stu-id="3a52c-166">The Dirichlet distribution is a multinomial distribution that describes the conditional probability of each variable in the network, and has many properties that are useful for learning.</span></span>  
  
 <span data-ttu-id="3a52c-167">El método Equivalente Dirichlet bayesiano con prioridad uniforme (BDEU) considera un caso especial de la distribución Dirichlet, en el que se utiliza una constante matemática para crear una distribución fija o uniforme de estados anteriores.</span><span class="sxs-lookup"><span data-stu-id="3a52c-167">The Bayesian Dirichlet Equivalent with Uniform Prior (BDEU) method assumes a special case of the Dirichlet distribution, in which a mathematical constant is used to create a fixed or uniform distribution of prior states.</span></span> <span data-ttu-id="3a52c-168">La puntuación BDE también considera la equivalencia de probabilidad; esto significa que no es de esperar que los datos diferencien estructuras equivalentes.</span><span class="sxs-lookup"><span data-stu-id="3a52c-168">The BDE score also assumes likelihood equivalence, which means that the data cannot be expected to discriminate equivalent structures.</span></span> <span data-ttu-id="3a52c-169">Es decir, si la puntuación de “Si A, entonces B” es la misma que la puntuación de “Si B, entonces A”, las estructuras no se pueden diferenciar basándose en los datos y no se puede deducir la causalidad.</span><span class="sxs-lookup"><span data-stu-id="3a52c-169">In other words, if the score for If A Then B is the same as the score for If B Then A, the structures cannot be distinguished based on the data, and causation cannot be inferred.</span></span>  
  
 <span data-ttu-id="3a52c-170">Para obtener más información sobre las redes bayesianas y la implementación de estos métodos de puntuación, vea este artículo sobre las [redes bayesianas](https://go.microsoft.com/fwlink/?LinkId=105885).</span><span class="sxs-lookup"><span data-stu-id="3a52c-170">For more information about Bayesian networks and the implementation of these scoring methods, see [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885).</span></span>  
  
### <a name="feature-selection-methods-used-by-analysis-services-algorithms"></a><span data-ttu-id="3a52c-171">Métodos de selección de características empleados por los algoritmos de Analysis Services</span><span class="sxs-lookup"><span data-stu-id="3a52c-171">Feature Selection Methods used by Analysis Services Algorithms</span></span>  
 <span data-ttu-id="3a52c-172">La tabla siguiente contiene una lista de los algoritmos que admiten la selección de características, los métodos de selección de características utilizados por los algoritmos, y los parámetros que se establecen para controlar el comportamiento de la selección de características.</span><span class="sxs-lookup"><span data-stu-id="3a52c-172">The following table lists the algorithms that support feature selection, the feature selection methods used by the algorithm, and the parameters that you set to control feature selection behavior:</span></span>  
  
|<span data-ttu-id="3a52c-173">Algoritmo</span><span class="sxs-lookup"><span data-stu-id="3a52c-173">Algorithm</span></span>|<span data-ttu-id="3a52c-174">Método de análisis</span><span class="sxs-lookup"><span data-stu-id="3a52c-174">Method of analysis</span></span>|<span data-ttu-id="3a52c-175">Comentarios</span><span class="sxs-lookup"><span data-stu-id="3a52c-175">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="3a52c-176">Bayes naive</span><span class="sxs-lookup"><span data-stu-id="3a52c-176">Naive Bayes</span></span>|<span data-ttu-id="3a52c-177">Entropía de Shannon</span><span class="sxs-lookup"><span data-stu-id="3a52c-177">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="3a52c-178">Bayesiano con prioridad K2</span><span class="sxs-lookup"><span data-stu-id="3a52c-178">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="3a52c-179">Dirichlet bayesiano con prioridad uniforme (predeterminado)</span><span class="sxs-lookup"><span data-stu-id="3a52c-179">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="3a52c-180">El algoritmo Bayes naive de Microsoft solo acepta atributos discretos o de datos discretos, por lo que no puede utilizar la puntuación interestingness.</span><span class="sxs-lookup"><span data-stu-id="3a52c-180">The Microsoft Naïve Bayes algorithm accepts only discrete or discretized attributes; therefore, it cannot use the interestingness score.</span></span><br /><br /> <span data-ttu-id="3a52c-181">Para obtener más información acerca de este algoritmo, vea [Microsoft Naive Bayes Algorithm Technical Reference](microsoft-naive-bayes-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="3a52c-181">For more information about this algorithm, see [Microsoft Naive Bayes Algorithm Technical Reference](microsoft-naive-bayes-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="3a52c-182">Árboles de decisión</span><span class="sxs-lookup"><span data-stu-id="3a52c-182">Decision trees</span></span>|<span data-ttu-id="3a52c-183">Puntuación interestingness</span><span class="sxs-lookup"><span data-stu-id="3a52c-183">Interestingness score</span></span><br /><br /> <span data-ttu-id="3a52c-184">Entropía de Shannon</span><span class="sxs-lookup"><span data-stu-id="3a52c-184">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="3a52c-185">Bayesiano con prioridad K2</span><span class="sxs-lookup"><span data-stu-id="3a52c-185">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="3a52c-186">Dirichlet bayesiano con prioridad uniforme (predeterminado)</span><span class="sxs-lookup"><span data-stu-id="3a52c-186">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="3a52c-187">Si alguna columna contiene valores continuos no binarios, se utiliza la puntuación interestingness (grado de interés) en todas las columnas para asegurar la coherencia.</span><span class="sxs-lookup"><span data-stu-id="3a52c-187">If any columns contain non-binary continuous values, the interestingness score is used for all columns, to ensure consistency.</span></span> <span data-ttu-id="3a52c-188">De lo contrario, se usa el método de selección de características predeterminado o el método que se haya especificado al crear el modelo.</span><span class="sxs-lookup"><span data-stu-id="3a52c-188">Otherwise, the default feature selection method is used, or the method that you specified when you created the model.</span></span><br /><br /> <span data-ttu-id="3a52c-189">Para obtener más información acerca de este algoritmo, vea [Microsoft Decision Trees Algorithm Technical Reference](microsoft-decision-trees-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="3a52c-189">For more information about this algorithm, see [Microsoft Decision Trees Algorithm Technical Reference](microsoft-decision-trees-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="3a52c-190">Red neuronal</span><span class="sxs-lookup"><span data-stu-id="3a52c-190">Neural network</span></span>|<span data-ttu-id="3a52c-191">Puntuación interestingness</span><span class="sxs-lookup"><span data-stu-id="3a52c-191">Interestingness score</span></span><br /><br /> <span data-ttu-id="3a52c-192">Entropía de Shannon</span><span class="sxs-lookup"><span data-stu-id="3a52c-192">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="3a52c-193">Bayesiano con prioridad K2</span><span class="sxs-lookup"><span data-stu-id="3a52c-193">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="3a52c-194">Dirichlet bayesiano con prioridad uniforme (predeterminado)</span><span class="sxs-lookup"><span data-stu-id="3a52c-194">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="3a52c-195">El algoritmo de redes neuronales de Microsoft puede usar el método bayesiano y el método basado en la entropía, siempre y cuando los datos contengan columnas continuas.</span><span class="sxs-lookup"><span data-stu-id="3a52c-195">The Microsoft Neural Networks algorithm can use both Bayesian and entropy-based methods, as long as the data contains continuous columns.</span></span><br /><br /> <span data-ttu-id="3a52c-196">Para obtener más información acerca de este algoritmo, vea [Microsoft Neural Network Algorithm Technical Reference](microsoft-neural-network-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="3a52c-196">For more information about this algorithm, see [Microsoft Neural Network Algorithm Technical Reference](microsoft-neural-network-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="3a52c-197">Regresión logística</span><span class="sxs-lookup"><span data-stu-id="3a52c-197">Logistic regression</span></span>|<span data-ttu-id="3a52c-198">Puntuación interestingness</span><span class="sxs-lookup"><span data-stu-id="3a52c-198">Interestingness score</span></span><br /><br /> <span data-ttu-id="3a52c-199">Entropía de Shannon</span><span class="sxs-lookup"><span data-stu-id="3a52c-199">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="3a52c-200">Bayesiano con prioridad K2</span><span class="sxs-lookup"><span data-stu-id="3a52c-200">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="3a52c-201">Dirichlet bayesiano con prioridad uniforme (predeterminado)</span><span class="sxs-lookup"><span data-stu-id="3a52c-201">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="3a52c-202">Aunque el algoritmo de regresión logística de Microsoft se basa en el algoritmo de red neuronal de Microsoft, no se pueden personalizar los modelos de regresión logística para controlar el comportamiento de la selección de características; por lo tanto, la selección de características siempre usa de manera predeterminada el método más apropiado para el atributo.</span><span class="sxs-lookup"><span data-stu-id="3a52c-202">Although the Microsoft Logistic Regression algorithm is based on the Microsoft Neural Network algorithm, you cannot customize logistic regression models to control feature selection behavior; therefore, feature selection always default to the method that is most appropriate for the attribute.</span></span><br /><br /> <span data-ttu-id="3a52c-203">Si todos los atributos son discretos o de datos discretos, el valor predeterminado es BDEU.</span><span class="sxs-lookup"><span data-stu-id="3a52c-203">If all attributes are discrete or discretized, the default is BDEU.</span></span><br /><br /> <span data-ttu-id="3a52c-204">Para obtener más información acerca de este algoritmo, vea [Microsoft Logistic Regression Algorithm Technical Reference](microsoft-logistic-regression-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="3a52c-204">For more information about this algorithm, see [Microsoft Logistic Regression Algorithm Technical Reference](microsoft-logistic-regression-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="3a52c-205">Agrupación en clústeres</span><span class="sxs-lookup"><span data-stu-id="3a52c-205">Clustering</span></span>|<span data-ttu-id="3a52c-206">Puntuación interestingness</span><span class="sxs-lookup"><span data-stu-id="3a52c-206">Interestingness score</span></span>|<span data-ttu-id="3a52c-207">El algoritmo de clústeres de Microsoft puede usar datos discretos o discretizados.</span><span class="sxs-lookup"><span data-stu-id="3a52c-207">The Microsoft Clustering algorithm can use discrete or discretized data.</span></span> <span data-ttu-id="3a52c-208">Sin embargo, dado que la puntuación de cada atributo se calcula como una distancia y se representa como un número continuo, se debe usar la puntuación interestingness.</span><span class="sxs-lookup"><span data-stu-id="3a52c-208">However, because the score of each attribute is calculated as a distance and is represented as a continuous number, the interestingness score must be used.</span></span><br /><br /> <span data-ttu-id="3a52c-209">Para obtener más información acerca de este algoritmo, vea [Microsoft Clustering Algorithm Technical Reference](microsoft-clustering-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="3a52c-209">For more information about this algorithm, see [Microsoft Clustering Algorithm Technical Reference](microsoft-clustering-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="3a52c-210">Regresión lineal</span><span class="sxs-lookup"><span data-stu-id="3a52c-210">Linear regression</span></span>|<span data-ttu-id="3a52c-211">Puntuación interestingness</span><span class="sxs-lookup"><span data-stu-id="3a52c-211">Interestingness score</span></span>|<span data-ttu-id="3a52c-212">El algoritmo de regresión lineal de Microsoft solo puede usar la puntuación interestingness porque solamente admite columnas continuas.</span><span class="sxs-lookup"><span data-stu-id="3a52c-212">The Microsoft Linear Regression algorithm can only use the interestingness score, because it only supports continuous columns.</span></span><br /><br /> <span data-ttu-id="3a52c-213">Para obtener más información acerca de este algoritmo, vea [Microsoft Linear Regression Algorithm Technical Reference](microsoft-linear-regression-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="3a52c-213">For more information about this algorithm, see [Microsoft Linear Regression Algorithm Technical Reference](microsoft-linear-regression-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="3a52c-214">Reglas de asociación</span><span class="sxs-lookup"><span data-stu-id="3a52c-214">Association rules</span></span><br /><br /> <span data-ttu-id="3a52c-215">Agrupación en clústeres de secuencia</span><span class="sxs-lookup"><span data-stu-id="3a52c-215">Sequence clustering</span></span>|<span data-ttu-id="3a52c-216">No se usa</span><span class="sxs-lookup"><span data-stu-id="3a52c-216">Not used</span></span>|<span data-ttu-id="3a52c-217">La selección de características no se invoca con estos algoritmos.</span><span class="sxs-lookup"><span data-stu-id="3a52c-217">Feature selection is not invoked with these algorithms.</span></span><br /><br /> <span data-ttu-id="3a52c-218">Sin embargo, se puede controlar el comportamiento del algoritmo y reducir el tamaño de los datos de entrada, si es necesario, al establecer el valor de los parámetros MINIMUM_SUPPORT y MINIMUM_PROBABILITY.</span><span class="sxs-lookup"><span data-stu-id="3a52c-218">However, you can control the behavior of the algorithm and reduce the size of input data if necessary by setting the value of the parameters MINIMUM_SUPPORT and MINIMUM_PROBABILIITY.</span></span><br /><br /> <span data-ttu-id="3a52c-219">Para obtener más información, consulte [Microsoft Association Algorithm Technical Reference](microsoft-association-algorithm-technical-reference.md) y [Microsoft Sequence Clustering Algorithm Technical Reference](microsoft-sequence-clustering-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="3a52c-219">For more information, see [Microsoft Association Algorithm Technical Reference](microsoft-association-algorithm-technical-reference.md) and [Microsoft Sequence Clustering Algorithm Technical Reference](microsoft-sequence-clustering-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="3a52c-220">Serie temporal</span><span class="sxs-lookup"><span data-stu-id="3a52c-220">Time series</span></span>|<span data-ttu-id="3a52c-221">No se usa</span><span class="sxs-lookup"><span data-stu-id="3a52c-221">Not used</span></span>|<span data-ttu-id="3a52c-222">La selección de características no se aplica a los modelos de serie temporal.</span><span class="sxs-lookup"><span data-stu-id="3a52c-222">Feature selection does not apply to time series models.</span></span><br /><br /> <span data-ttu-id="3a52c-223">Para obtener más información acerca de este algoritmo, vea [Microsoft Time Series Algorithm Technical Reference](microsoft-time-series-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="3a52c-223">For more information about this algorithm, see [Microsoft Time Series Algorithm Technical Reference](microsoft-time-series-algorithm-technical-reference.md).</span></span>|  
  
## <a name="feature-selection-parameters"></a><span data-ttu-id="3a52c-224">Parámetros de selección de características</span><span class="sxs-lookup"><span data-stu-id="3a52c-224">Feature Selection Parameters</span></span>  
 <span data-ttu-id="3a52c-225">En los algoritmos que admiten la selección de características se puede controlar cuándo se encuentra activada dicha selección usando los parámetros siguientes.</span><span class="sxs-lookup"><span data-stu-id="3a52c-225">In algorithms that support feature selection, you can control when feature selection is turned on by using the following parameters.</span></span> <span data-ttu-id="3a52c-226">Cada algoritmo tiene un valor predeterminado para el número de entradas permitidas, pero se puede invalidar este valor y especificar el número de atributos.</span><span class="sxs-lookup"><span data-stu-id="3a52c-226">Each algorithm has a default value for the number of inputs that are allowed, but you can override this default and specify the number of attributes.</span></span> <span data-ttu-id="3a52c-227">En esta sección se enumeran los parámetros que se proporcionan para administrar la selección de características.</span><span class="sxs-lookup"><span data-stu-id="3a52c-227">This section lists the parameters that are provided for managing feature selection.</span></span>  
  
#### <a name="maximum_input_attributes"></a><span data-ttu-id="3a52c-228">MAXIMUM_INPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="3a52c-228">MAXIMUM_INPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="3a52c-229">Si un modelo contiene más columnas que el número especificado en el parámetro *MAXIMUM_INPUT_ATTRIBUTES* , el algoritmo pasa por alto cualquier columna que determina como no interesante.</span><span class="sxs-lookup"><span data-stu-id="3a52c-229">If a model contains more columns than the number that is specified in the *MAXIMUM_INPUT_ATTRIBUTES* parameter, the algorithm ignores any columns that it calculates to be uninteresting.</span></span>  
  
#### <a name="maximum_output_attributes"></a><span data-ttu-id="3a52c-230">MAXIMUM_OUTPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="3a52c-230">MAXIMUM_OUTPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="3a52c-231">De forma similar, si un modelo contiene más columnas de predicción que el número especificado en el parámetro *MAXIMUM_OUTPUT_ATTRIBUTES* , el algoritmo pasa por alto cualquier columna que determina como no interesante.</span><span class="sxs-lookup"><span data-stu-id="3a52c-231">Similarly, if a model contains more predictable columns than the number that is specified in the *MAXIMUM_OUTPUT_ATTRIBUTES* parameter, the algorithm ignores any columns that it calculates to be uninteresting.</span></span>  
  
#### <a name="maximum_states"></a><span data-ttu-id="3a52c-232">MAXIMUM_STATES</span><span class="sxs-lookup"><span data-stu-id="3a52c-232">MAXIMUM_STATES</span></span>  
 <span data-ttu-id="3a52c-233">Si un modelo contiene más casos de los especificados en el parámetro *MAXIMUM_STATES* , los estados con menor popularidad se agrupan y se tratan como estados que faltan.</span><span class="sxs-lookup"><span data-stu-id="3a52c-233">If a model contains more cases than are specified in the *MAXIMUM_STATES* parameter, the least popular states are grouped together and treated as missing.</span></span> <span data-ttu-id="3a52c-234">Si alguno de estos parámetros se establece en 0, la selección de características se desactiva. Esto afecta al tiempo de procesamiento y al rendimiento.</span><span class="sxs-lookup"><span data-stu-id="3a52c-234">If any one of these parameters is set to 0, feature selection is turned off, affecting processing time and performance.</span></span>  
  
 <span data-ttu-id="3a52c-235">Además de estos métodos para la selección de características, es posible mejorar la capacidad del algoritmo para identificar o promover atributos significativos estableciendo *marcas de modelado* en el modelo o *marcas de distribución* en la estructura.</span><span class="sxs-lookup"><span data-stu-id="3a52c-235">In addition to these methods for feature selection, you can improve the ability of the algorithm to identify or promote meaningful attributes by setting *modeling flags* on the model or by setting *distribution flags* on the structure.</span></span> <span data-ttu-id="3a52c-236">Para más información sobre estos conceptos, vea [Marcas de modelado &#40;minería de datos&#41;](modeling-flags-data-mining.md) y [Distribuciones de columnas &#40;minería de datos&#41;](column-distributions-data-mining.md).</span><span class="sxs-lookup"><span data-stu-id="3a52c-236">For more information about these concepts, see [Modeling Flags &#40;Data Mining&#41;](modeling-flags-data-mining.md) and [Column Distributions &#40;Data Mining&#41;](column-distributions-data-mining.md).</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="3a52c-237">Consulte también</span><span class="sxs-lookup"><span data-stu-id="3a52c-237">See Also</span></span>  
 [<span data-ttu-id="3a52c-238">Personalizar la estructura y los modelos de minería de datos</span><span class="sxs-lookup"><span data-stu-id="3a52c-238">Customize Mining Models and Structure</span></span>](customize-mining-models-and-structure.md)  
  
  
