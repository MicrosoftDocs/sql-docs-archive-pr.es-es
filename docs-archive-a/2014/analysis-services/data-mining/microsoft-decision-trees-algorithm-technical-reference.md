---
title: Referencia técnica del algoritmo de árboles de decisión de Microsoft | Microsoft Docs
ms.custom: ''
ms.date: 06/13/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- MAXIMUM_INPUT_ATTRIBUTES parameter
- SPLIT_METHOD parameter
- MINIMUM_SUPPORT parameter
- MAXIMUM_OUTPUT_ATTRIBUTES parameter
- FORCED_REGRESSOR parameter
- decision tree algorithms [Analysis Services]
- decision trees [Analysis Services]
- COMPLEXITY_PENALTY parameter
- SCORE_METHOD parameter
ms.assetid: 1e9f7969-0aa6-465a-b3ea-57b8d1c7a1fd
author: minewiskan
ms.author: owend
ms.openlocfilehash: 0cd0cd3100d0ed1213183815ae41f17cee3baa68
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: es-ES
ms.lasthandoff: 08/04/2020
ms.locfileid: "87674438"
---
# <a name="microsoft-decision-trees-algorithm-technical-reference"></a><span data-ttu-id="ddc3f-102">Referencia técnica del algoritmo de árboles de decisión de Microsoft</span><span class="sxs-lookup"><span data-stu-id="ddc3f-102">Microsoft Decision Trees Algorithm Technical Reference</span></span>
  <span data-ttu-id="ddc3f-103">El algoritmo de árboles de decisión de [!INCLUDE[msCoName](../../includes/msconame-md.md)] es un algoritmo híbrido que incorpora distintos métodos para crear un árbol, y admite varias tareas de análisis, incluyendo la regresión, la clasificación y la asociación.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-103">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm is a hybrid algorithm that incorporates different methods for creating a tree, and supports multiple analytic tasks, including regression, classification, and association.</span></span> <span data-ttu-id="ddc3f-104">El algoritmo de árboles de decisión de Microsoft admite el modelado de los atributos discretos y continuos.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-104">The Microsoft Decision Trees algorithm supports modeling of both discrete and continuous attributes.</span></span>  
  
 <span data-ttu-id="ddc3f-105">En este tema se explica la implementación del algoritmo, se describe cómo personalizar su comportamiento para distintas tareas y se proporcionan vínculos a información adicional sobre cómo consultar los modelos de árboles de decisión.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-105">This topic explains the implementation of the algorithm, describes how to customize the behavior of the algorithm for different tasks, and provides links to additional information about querying decision tree models.</span></span>  
  
## <a name="implementation-of-the-decision-trees-algorithm"></a><span data-ttu-id="ddc3f-106">Implementación del algoritmo de árboles de decisión</span><span class="sxs-lookup"><span data-stu-id="ddc3f-106">Implementation of the Decision Trees Algorithm</span></span>  
 <span data-ttu-id="ddc3f-107">El algoritmo de árboles de decisión de Microsoft aplica el enfoque bayesiano en el aprendizaje de los modelos de interacción causales al obtener las distribuciones posteriores aproximadas de los modelos.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-107">The Microsoft Decision Trees algorithm applies the Bayesian approach to learning causal interaction models by obtaining approximate posterior distributions for the models.</span></span> <span data-ttu-id="ddc3f-108">Para obtener una explicación detallada de este enfoque, consulte el documento en el sitio de Microsoft Research sobre el [aprendizaje de la estructura y parámetros](https://go.microsoft.com/fwlink/?LinkId=237640&clcid=0x409).</span><span class="sxs-lookup"><span data-stu-id="ddc3f-108">For a detailed explanation of this approach, see the paper on the Microsoft Research site, by [Structure and Parameter Learning](https://go.microsoft.com/fwlink/?LinkId=237640&clcid=0x409).</span></span>  
  
 <span data-ttu-id="ddc3f-109">La metodología para evaluar el valor de la información de las *prioridades* necesarias para el aprendizaje se basa en el supuesto de *equivalencia de probabilidad*.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-109">The methodology for assessing the information value of the *priors* needed for learning is based on the assumption of *likelihood equivalence*.</span></span> <span data-ttu-id="ddc3f-110">Este supuesto establece que los datos no deberían ayudar a discriminar estructuras de red que, de otro modo, representarían las mismas aserciones de independencia condicional.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-110">This assumption says that data should not help to discriminate network structures that otherwise represent the same assertions of conditional independence.</span></span> <span data-ttu-id="ddc3f-111">Se supone que cada caso tiene una única red bayesiana anterior y una única medida de confianza para dicha red.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-111">Each case is assumed to have a single Bayesian prior network and a single measure of confidence for that network.</span></span>  
  
 <span data-ttu-id="ddc3f-112">Mediante estas redes anteriores, el algoritmo calcula las *probabilidades posteriores* relativas de las estructuras de red dados los datos de entrenamiento actuales, e identifica las estructuras de red con las probabilidades posteriores más altas.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-112">Using these prior networks, the algorithm then computes the relative *posterior probabilities* of network structures given the current training data, and identifies the network structures that have the highest posterior probabilities.</span></span>  
  
 <span data-ttu-id="ddc3f-113">El algoritmo de árboles de decisión de Microsoft usa distintos métodos para calcular el mejor árbol.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-113">The Microsoft Decision Trees algorithm uses different methods to compute the best tree.</span></span> <span data-ttu-id="ddc3f-114">El método usado dependerá de la tarea, que puede ser la regresión lineal, la clasificación o el análisis de la asociación.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-114">The method used depends on the task, which can be linear regression, classification, or association analysis.</span></span> <span data-ttu-id="ddc3f-115">Un solo modelo puede contener varios árboles para distintos atributos de predicción.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-115">A single model can contain multiple trees for different predictable attributes.</span></span> <span data-ttu-id="ddc3f-116">Es más, cada árbol puede contener varias bifurcaciones, dependiendo del número de atributos y de valores que contienen los datos.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-116">Moreover, each tree can contain multiple branches, depending on how many attributes and values there are in the data.</span></span> <span data-ttu-id="ddc3f-117">La forma y profundidad del árbol integrado en un modelo determinado depende del método de puntuación y del resto de parámetros usados.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-117">The shape and depth of the tree built in a particular model depends on the scoring method and other parameters that were used.</span></span> <span data-ttu-id="ddc3f-118">Los cambios en los parámetros también pueden afectar al lugar donde se dividen los nodos.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-118">Changes in the parameters can also affect where the nodes split.</span></span>  
  
### <a name="building-the-tree"></a><span data-ttu-id="ddc3f-119">Generar el árbol</span><span class="sxs-lookup"><span data-stu-id="ddc3f-119">Building the Tree</span></span>  
 <span data-ttu-id="ddc3f-120">Cuando el algoritmo de árboles de decisión de Microsoft crea el conjunto de posibles valores de entrada, realiza una *feature selection* para identificar los atributos y los valores que ofrecen la mayor cantidad de información, y no tiene en cuenta los valores que son muy raros.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-120">When the Microsoft Decision Trees algorithm creates the set of possible input values, it performs *feature selection* to identify the attributes and values that provide the most information, and removes from consideration the values that are very rare.</span></span> <span data-ttu-id="ddc3f-121">El algoritmo también agrupa los valores en *bandejas*para crear agrupaciones de valores que se pueden procesar como una unidad para optimizar el rendimiento.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-121">The algorithm also groups values into *bins*, to create groupings of values that can be processed as a unit to optimize performance.</span></span>  
  
 <span data-ttu-id="ddc3f-122">Un árbol se genera mediante la determinación de las correlaciones entre una entrada y el resultado deseado.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-122">A tree is built by determining the correlations between an input and the targeted outcome.</span></span> <span data-ttu-id="ddc3f-123">Una vez correlacionados todos los atributos, el algoritmo identifica el atributo único que separa más claramente los resultados.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-123">After all the attributes have been correlated, the algorithm identifies the single attribute that most cleanly separates the outcomes.</span></span> <span data-ttu-id="ddc3f-124">Este punto de la mejor separación se mide usando una ecuación que calcula la obtención de información.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-124">This point of the best separation is measured by using an equation that calculates information gain.</span></span> <span data-ttu-id="ddc3f-125">El atributo que tiene la mejor puntuación para la obtención de información se usa para dividir los casos en subconjuntos, que posteriormente son analizados de forma recursiva por el mismo proceso hasta que no sea posible dividir más el árbol.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-125">The attribute that has the best score for information gain is used to divide the cases into subsets, which are then recursively analyzed by the same process, until the tree cannot be split any more.</span></span>  
  
 <span data-ttu-id="ddc3f-126">La ecuación exacta empleada para evaluar la obtención de información depende de los parámetros establecidos al crear el algoritmo, del tipo de datos de la columna de predicción y del tipo de datos de la entrada.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-126">The exact equation used to evaluate information gain depends on the parameters set when you created the algorithm, the data type of the predictable column, and the data type of the input.</span></span>  
  
### <a name="discrete-and-continuous-inputs"></a><span data-ttu-id="ddc3f-127">Entradas discretas y continuas</span><span class="sxs-lookup"><span data-stu-id="ddc3f-127">Discrete and Continuous Inputs</span></span>  
 <span data-ttu-id="ddc3f-128">Cuando tanto el atributo de predicción como las entradas son discretas, el recuento de los resultados por entrada se realizará creando una matriz y generando puntuaciones para cada celda de la matriz.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-128">When the predictable attribute is discrete and the inputs are discrete, counting the outcomes per input is a matter of creating a matrix and generating scores for each cell in the matrix.</span></span>  
  
 <span data-ttu-id="ddc3f-129">Sin embargo, cuando el atributo de predicción es discreto y las entradas son continuas, la entrada de las columnas continuas se discretiza automáticamente.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-129">However, when the predictable attribute is discrete and the inputs are continuous, the input of the continuous columns are automatically discretized.</span></span> <span data-ttu-id="ddc3f-130">Puede aceptar los valores predeterminados para que [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] determine el número óptimo de agrupaciones, o bien puede controlar la forma en que las entradas continuas se discretizan si establece las propiedades <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationMethod%2A> y <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationBucketCount%2A> .</span><span class="sxs-lookup"><span data-stu-id="ddc3f-130">You can accept the default and have [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] find the optimum number of bins, or you can control the manner in which continuous inputs are discretized by setting the <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationMethod%2A> and <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationBucketCount%2A> properties.</span></span> <span data-ttu-id="ddc3f-131">Para más información, vea [Cambiar la discretización de una columna en un modelo de minería de datos](change-the-discretization-of-a-column-in-a-mining-model.md).</span><span class="sxs-lookup"><span data-stu-id="ddc3f-131">For more information, see [Change the Discretization of a Column in a Mining Model](change-the-discretization-of-a-column-in-a-mining-model.md).</span></span>  
  
 <span data-ttu-id="ddc3f-132">Para los atributos continuos, el algoritmo usa la regresión lineal para determinar dónde se divide un árbol de decisión.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-132">For continuous attributes, the algorithm uses linear regression to determine where a decision tree splits.</span></span>  
  
 <span data-ttu-id="ddc3f-133">Cuando el atributo de predicción es un tipo de datos numéricos continuo, la selección de características también se aplica a las salidas para reducir el número de resultados posibles y generar más rápidamente el modelo.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-133">When the predictable attribute is a continuous numeric data type, feature selection is applied to the outputs as well, to reduce the possible number of outcomes and build the model faster.</span></span> <span data-ttu-id="ddc3f-134">Puede cambiar el umbral para la selección de características, incrementando o disminuyendo de esta manera el número de valores posibles, estableciendo el parámetro MAXIMUM_OUTPUT_ATTRIBUTES.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-134">You can change the threshold for feature selection and thereby increase or decrease the number of possible values by setting the MAXIMUM_OUTPUT_ATTRIBUTES parameter.</span></span>  
  
 <span data-ttu-id="ddc3f-135">Para obtener una explicación más detallada acerca de cómo funciona el algoritmo de árboles de decisión de [!INCLUDE[msCoName](../../includes/msconame-md.md)] con columnas de predicción discretas, vea el artículo sobre [Descripción de las redes bayesianas: combinación de conocimiento y datos estadísticos](https://go.microsoft.com/fwlink/?LinkId=45963).</span><span class="sxs-lookup"><span data-stu-id="ddc3f-135">For a more detained explanation about how the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm works with discrete predictable columns, see [Learning Bayesian Networks: The Combination of Knowledge and Statistical Data](https://go.microsoft.com/fwlink/?LinkId=45963).</span></span> <span data-ttu-id="ddc3f-136">Para más información sobre cómo funciona el algoritmo de árboles de decisión de [!INCLUDE[msCoName](../../includes/msconame-md.md)] con una columna de predicción continua, vea el apéndice del artículo [Autoregressive Tree Models for Time-Series Analysis](https://go.microsoft.com/fwlink/?LinkId=45966)(Modelos de árbol de regresión automática para el análisis de series temporales).</span><span class="sxs-lookup"><span data-stu-id="ddc3f-136">For more information about how the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm works with a continuous predictable column, see the appendix of [Autoregressive Tree Models for Time-Series Analysis](https://go.microsoft.com/fwlink/?LinkId=45966).</span></span>  
  
### <a name="scoring-methods-and-feature-selection"></a><span data-ttu-id="ddc3f-137">Métodos de puntuación y selección de características</span><span class="sxs-lookup"><span data-stu-id="ddc3f-137">Scoring Methods and Feature Selection</span></span>  
 <span data-ttu-id="ddc3f-138">El algoritmo de árboles de decisión de Microsoft proporciona tres fórmulas para puntuar la obtención de información: la entropía de Shannon, la red bayesiana con prioridad K2 y la red bayesiana con una distribución Dirichlet uniforme de prioridades.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-138">The Microsoft Decision Trees algorithm offers three formulas for scoring information gain: Shannon's entropy, Bayesian network with K2 prior, and Bayesian network with a uniform Dirichlet distribution of priors.</span></span> <span data-ttu-id="ddc3f-139">Los tres métodos están bien consolidados en el campo de la minería de datos.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-139">All three methods are well established in the data mining field.</span></span> <span data-ttu-id="ddc3f-140">Se recomienda que experimente con parámetros y métodos de puntuación diferentes para determinar cuáles son los que proporcionan mejores resultados.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-140">We recommend that you experiment with different parameters and scoring methods to determine which provides the best results.</span></span> <span data-ttu-id="ddc3f-141">Para obtener más información acerca de estos métodos de puntuación, vea [Feature Selection](../../sql-server/install/feature-selection.md).</span><span class="sxs-lookup"><span data-stu-id="ddc3f-141">For more information about these scoring methods, see [Feature Selection](../../sql-server/install/feature-selection.md).</span></span>  
  
 <span data-ttu-id="ddc3f-142">Todos los algoritmos de minería de datos de [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] usan automáticamente la selección de características para mejorar el análisis y reducir la carga de procesamiento.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-142">All [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] data mining algorithms automatically use feature selection to improve analysis and reduce processing load.</span></span> <span data-ttu-id="ddc3f-143">El método usado para la selección de características depende del algoritmo empleado para generar el modelo.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-143">The method used for feature selection depends on the algorithm that is used to build the model.</span></span> <span data-ttu-id="ddc3f-144">Los parámetros del algoritmo que controlan la selección de características para el modelo de árboles de decisión son MAXIMUM_INPUT_ATTRIBUTES y MAXIMUM_OUTPUT.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-144">The algorithm parameters that control feature selection for a decision trees model are MAXIMUM_INPUT_ATTRIBUTES and MAXIMUM_OUTPUT.</span></span>  
  
|<span data-ttu-id="ddc3f-145">Algoritmo</span><span class="sxs-lookup"><span data-stu-id="ddc3f-145">Algorithm</span></span>|<span data-ttu-id="ddc3f-146">Método de análisis</span><span class="sxs-lookup"><span data-stu-id="ddc3f-146">Method of analysis</span></span>|<span data-ttu-id="ddc3f-147">Comentarios</span><span class="sxs-lookup"><span data-stu-id="ddc3f-147">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="ddc3f-148">Árboles de decisión</span><span class="sxs-lookup"><span data-stu-id="ddc3f-148">Decision Trees</span></span>|<span data-ttu-id="ddc3f-149">Puntuación interestingness</span><span class="sxs-lookup"><span data-stu-id="ddc3f-149">Interestingness score</span></span><br /><br /> <span data-ttu-id="ddc3f-150">Entropía de Shannon</span><span class="sxs-lookup"><span data-stu-id="ddc3f-150">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="ddc3f-151">Bayesiano con prioridad K2</span><span class="sxs-lookup"><span data-stu-id="ddc3f-151">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="ddc3f-152">Dirichlet bayesiano con prioridad uniforme (predeterminado)</span><span class="sxs-lookup"><span data-stu-id="ddc3f-152">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="ddc3f-153">Si alguna columna contiene valores continuos no binarios, se utiliza la puntuación interestingness (grado de interés) en todas las columnas para asegurar la coherencia.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-153">If any columns contain non-binary continuous values, the interestingness score is used for all columns, to ensure consistency.</span></span> <span data-ttu-id="ddc3f-154">En caso contrario, se utiliza el método predeterminado o el especificado.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-154">Otherwise, the default or specified method is used.</span></span>|  
|<span data-ttu-id="ddc3f-155">Regresión lineal</span><span class="sxs-lookup"><span data-stu-id="ddc3f-155">Linear Regression</span></span>|<span data-ttu-id="ddc3f-156">Puntuación interestingness</span><span class="sxs-lookup"><span data-stu-id="ddc3f-156">Interestingness score</span></span>|<span data-ttu-id="ddc3f-157">La regresión lineal solo utiliza la puntuación interestingness porque solo admite columnas continuas.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-157">Linear Regression only uses interestingness, because it only supports continuous columns.</span></span>|  
  
### <a name="scalability-and-performance"></a><span data-ttu-id="ddc3f-158">Escalabilidad y rendimiento</span><span class="sxs-lookup"><span data-stu-id="ddc3f-158">Scalability and Performance</span></span>  
 <span data-ttu-id="ddc3f-159">La clasificación es una estrategia de minería de datos importante.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-159">Classification is an important data mining strategy.</span></span> <span data-ttu-id="ddc3f-160">Generalmente, la cantidad de información necesaria para clasificar los casos crece en proporción directa al número de registros de entrada.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-160">Generally, the amount of information that is needed to classify the cases grows in direct proportion to the number of input records.</span></span> <span data-ttu-id="ddc3f-161">Esto limita el tamaño de los datos que se pueden clasificar.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-161">This limits the size of the data that can be classified.</span></span> <span data-ttu-id="ddc3f-162">El algoritmo de árboles de decisión de Microsoft usa los métodos siguientes para resolver estos problemas, mejorar el rendimiento y eliminar las restricciones de memoria:</span><span class="sxs-lookup"><span data-stu-id="ddc3f-162">The Microsoft Decision Trees algorithm using uses the following methods to resolve these problems, improve performance, and eliminate memory restrictions:</span></span>  
  
-   <span data-ttu-id="ddc3f-163">Selección de características para optimizar la selección de atributos.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-163">Feature selection to optimize the selection of attributes.</span></span>  
  
-   <span data-ttu-id="ddc3f-164">Puntuación bayesiana para controlar el crecimiento del árbol.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-164">Bayesian scoring to control tree growth.</span></span>  
  
-   <span data-ttu-id="ddc3f-165">Optimización de bandejas para los atributos continuos.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-165">Optimization of binning for continuous attributes.</span></span>  
  
-   <span data-ttu-id="ddc3f-166">Agrupación dinámica de valores de entrada para determinar los valores más importantes.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-166">Dynamic grouping of input values to determine the most important values.</span></span>  
  
 <span data-ttu-id="ddc3f-167">El algoritmo de árboles de decisión de Microsoft es rápido y escalable, y se ha diseñado para ser usado en paralelo, es decir, con todos los procesadores funcionando juntos para generar un modelo único y coherente.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-167">The Microsoft Decision Trees algorithm is fast and scalable, and has been designed to be easily parallelized, meaning that all processors work together to build a single, consistent model.</span></span> <span data-ttu-id="ddc3f-168">La combinación de estas características convierte al clasificador de árboles de decisión en una herramienta ideal para la minería de datos.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-168">The combination of these characteristics makes the decision-tree classifier an ideal tool for data mining.</span></span>  
  
 <span data-ttu-id="ddc3f-169">Si las restricciones de rendimiento son graves, podría mejorar el tiempo de procesamiento durante el entrenamiento de un modelo de árbol de decisión usando los métodos siguientes.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-169">If performance constraints are severe, you might be able to improve processing time during the training of a decision tree model by using the following methods.</span></span> <span data-ttu-id="ddc3f-170">Sin embargo, si lo hace, tenga en cuenta que la eliminación de atributos para mejorar el rendimiento del procesamiento cambiará los resultados del modelo, y es posible que éste sea menos representativo de la población total.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-170">However, if you do so, be aware that eliminating attributes to improve processing performance will change the results of the model, and possibly make it less representative of the total population.</span></span>  
  
-   <span data-ttu-id="ddc3f-171">Aumente el valor del parámetro COMPLEXITY_PENALTY para limitar el crecimiento del árbol.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-171">Increase the value of the COMPLEXITY_PENALTY parameter to limit tree growth.</span></span>  
  
-   <span data-ttu-id="ddc3f-172">Limite el número de elementos de los modelos de asociación para limitar el número de árboles que se generan.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-172">Limit the number of items in association models to limit the number of trees that are built.</span></span>  
  
-   <span data-ttu-id="ddc3f-173">Aumente el valor del parámetro MINIMUM_SUPPORT para evitar el sobreajuste.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-173">Increase the value of the MINIMUM_SUPPORT parameter to avoid overfitting.</span></span>  
  
-   <span data-ttu-id="ddc3f-174">Restrinja a 10 o menos el número de valores discretos para todos los atributos.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-174">Restrict the number of discrete values for any attribute to 10 or less.</span></span> <span data-ttu-id="ddc3f-175">Puede intentar agrupar valores de distintas maneras en modelos diferentes.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-175">You might try grouping values in different ways in different models.</span></span>  
  
    > [!NOTE]  
    >  <span data-ttu-id="ddc3f-176">Puede utilizar las herramientas de exploración de datos disponibles en  [!INCLUDE[ssISCurrent](../../includes/ssiscurrent-md.md)] para visualizar la distribución de valores en los datos y agrupar de forma apropiada dichos valores antes de comenzar la minería de datos.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-176">You can use the data exploration tools available in  [!INCLUDE[ssISCurrent](../../includes/ssiscurrent-md.md)] to visualize the distribution of values in your data and group your values appropriately before beginning data mining.</span></span> <span data-ttu-id="ddc3f-177">Para más información, vea [Tarea de generación de perfiles de datos y Visor](../../integration-services/control-flow/data-profiling-task-and-viewer.md).</span><span class="sxs-lookup"><span data-stu-id="ddc3f-177">For more information, see [Data Profiling Task and Viewer](../../integration-services/control-flow/data-profiling-task-and-viewer.md).</span></span> <span data-ttu-id="ddc3f-178">También puede usar los [complementos de minería de datos para Excel 2007](https://www.microsoft.com/download/details.aspx?id=8569)para explorar y agrupar datos en Microsoft Excel, así como para cambiar sus etiquetas.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-178">You can also use the [Data Mining Add-ins for Excel 2007](https://www.microsoft.com/download/details.aspx?id=8569), to explore, group and relabel data in Microsoft Excel.</span></span>  
  
## <a name="customizing-the-decision-trees-algorithm"></a><span data-ttu-id="ddc3f-179">Personalizar el algoritmo de árboles de decisión</span><span class="sxs-lookup"><span data-stu-id="ddc3f-179">Customizing the Decision Trees Algorithm</span></span>  
 <span data-ttu-id="ddc3f-180">El algoritmo de árboles de decisión de [!INCLUDE[msCoName](../../includes/msconame-md.md)] admite parámetros que afectan al rendimiento y la precisión del modelo de minería de datos resultante.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-180">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm supports parameters that affect the performance and accuracy of the resulting mining model.</span></span> <span data-ttu-id="ddc3f-181">También puede establecer marcas de modelado en las columnas del modelo de minería de datos o de la estructura de minería de datos para controlar la manera en que se procesan los datos.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-181">You can also set modeling flags on the mining model columns or mining structure columns to control the way that data is processed.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="ddc3f-182">El algoritmo de árboles de decisión de Microsoft está disponible en todas las ediciones de [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]; sin embargo, algunos parámetros avanzados para personalizar el comportamiento de dicho algoritmo pueden usarse exclusivamente en ciertas ediciones de [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)].</span><span class="sxs-lookup"><span data-stu-id="ddc3f-182">The Microsoft Decision Trees algorithm is available in all editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]; however, some advanced parameters for customizing the behavior of the Microsoft Decision Trees algorithm are available for use only in specific editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)].</span></span> <span data-ttu-id="ddc3f-183">Para obtener una lista de las características admitidas por las ediciones de [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)] , vea [características compatibles con las ediciones de SQL Server 2012](https://go.microsoft.com/fwlink/?linkid=232473) ( https://go.microsoft.com/fwlink/?linkid=232473) .</span><span class="sxs-lookup"><span data-stu-id="ddc3f-183">For a list of features that are supported by the editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)], see [Features Supported by the Editions of SQL Server 2012](https://go.microsoft.com/fwlink/?linkid=232473) (https://go.microsoft.com/fwlink/?linkid=232473).</span></span>  
  
### <a name="setting-algorithm-parameters"></a><span data-ttu-id="ddc3f-184">Establecer parámetros del algoritmo</span><span class="sxs-lookup"><span data-stu-id="ddc3f-184">Setting Algorithm Parameters</span></span>  
 <span data-ttu-id="ddc3f-185">En la tabla siguiente se describen los parámetros que puede usar con el algoritmo de árboles de decisión de [!INCLUDE[msCoName](../../includes/msconame-md.md)] .</span><span class="sxs-lookup"><span data-stu-id="ddc3f-185">The following table describes the parameters that you can use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm.</span></span>  
  
 <span data-ttu-id="ddc3f-186">*COMPLEXITY_PENALTY*</span><span class="sxs-lookup"><span data-stu-id="ddc3f-186">*COMPLEXITY_PENALTY*</span></span>  
 <span data-ttu-id="ddc3f-187">Controla el crecimiento del árbol de decisión.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-187">Controls the growth of the decision tree.</span></span> <span data-ttu-id="ddc3f-188">Un valor bajo aumenta el número de divisiones y un valor alto lo reduce.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-188">A low value increases the number of splits, and a high value decreases the number of splits.</span></span> <span data-ttu-id="ddc3f-189">El valor predeterminado se basa en el número de atributos de un modelo concreto, como se describe en la lista siguiente:</span><span class="sxs-lookup"><span data-stu-id="ddc3f-189">The default value is based on the number of attributes for a particular model, as described in the following list:</span></span>  
  
-   <span data-ttu-id="ddc3f-190">De 1 a 9 atributos, el valor predeterminado es 0,5.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-190">For 1 through 9 attributes, the default is 0.5.</span></span>  
  
-   <span data-ttu-id="ddc3f-191">De 10 a 99 atributos, el valor predeterminado es 0,9.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-191">For 10 through 99 attributes, the default is 0.9.</span></span>  
  
-   <span data-ttu-id="ddc3f-192">Para 100 o más atributos, el valor predeterminado es 0,99.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-192">For 100 or more attributes, the default is 0.99.</span></span>  
  
 <span data-ttu-id="ddc3f-193">*FORCE_REGRESSOR*</span><span class="sxs-lookup"><span data-stu-id="ddc3f-193">*FORCE_REGRESSOR*</span></span>  
 <span data-ttu-id="ddc3f-194">Fuerza al algoritmo a utilizar las columnas indicadas como regresores, independientemente de su importancia según los cálculos del algoritmo.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-194">Forces the algorithm to use the specified columns as regressors, regardless of the importance of the columns as calculated by the algorithm.</span></span> <span data-ttu-id="ddc3f-195">Este parámetro sólo se usa para árboles de decisión que predicen un atributo continuo.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-195">This parameter is only used for decision trees that are predicting a continuous attribute.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="ddc3f-196">Establezca este parámetro si desea que el algoritmo intente usar el atributo como un regresor.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-196">By setting this parameter, you force the algorithm to try to use the attribute as a regressor.</span></span> <span data-ttu-id="ddc3f-197">Sin embargo, el atributo se usará realmente como regresor en el modelo final en función de los resultados del análisis.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-197">However, whether the attribute is actually used as a regressor in the final model depends on the results of analysis.</span></span> <span data-ttu-id="ddc3f-198">Para averiguar las columnas que se usaron como regresores, consulte el contenido del modelo.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-198">You can find out which columns were used as regressors by querying the model content.</span></span>  
  
 <span data-ttu-id="ddc3f-199">[Disponible solo en algunas ediciones de [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)] ]</span><span class="sxs-lookup"><span data-stu-id="ddc3f-199">[Available only in some editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)] ]</span></span>  
  
 <span data-ttu-id="ddc3f-200">*MAXIMUM_INPUT_ATTRIBUTES*</span><span class="sxs-lookup"><span data-stu-id="ddc3f-200">*MAXIMUM_INPUT_ATTRIBUTES*</span></span>  
 <span data-ttu-id="ddc3f-201">Define el número de atributos de entrada que el algoritmo puede controlar antes de invocar la selección de características.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-201">Defines the number of input attributes that the algorithm can handle before it invokes feature selection.</span></span>  
  
 <span data-ttu-id="ddc3f-202">El valor predeterminado es 255.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-202">The default is 255.</span></span>  
  
 <span data-ttu-id="ddc3f-203">Establezca este valor en 0 para desactivar la selección de características.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-203">Set this value to 0 to turn off feature selection.</span></span>  
  
 <span data-ttu-id="ddc3f-204">[Disponible solo en algunas ediciones de [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]]</span><span class="sxs-lookup"><span data-stu-id="ddc3f-204">[Available only in some editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]]</span></span>  
  
 <span data-ttu-id="ddc3f-205">*MAXIMUM_OUTPUT_ATTRIBUTES*</span><span class="sxs-lookup"><span data-stu-id="ddc3f-205">*MAXIMUM_OUTPUT_ATTRIBUTES*</span></span>  
 <span data-ttu-id="ddc3f-206">Define el número de atributos de salida que el algoritmo puede controlar antes de invocar la selección de características.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-206">Defines the number of output attributes that the algorithm can handle before it invokes feature selection.</span></span>  
  
 <span data-ttu-id="ddc3f-207">El valor predeterminado es 255.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-207">The default is 255.</span></span>  
  
 <span data-ttu-id="ddc3f-208">Establezca este valor en 0 para desactivar la selección de características.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-208">Set this value to 0 to turn off feature selection.</span></span>  
  
 <span data-ttu-id="ddc3f-209">[Disponible solo en algunas ediciones de [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]]</span><span class="sxs-lookup"><span data-stu-id="ddc3f-209">[Available only in some editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]]</span></span>  
  
 <span data-ttu-id="ddc3f-210">*MINIMUM_SUPPORT*</span><span class="sxs-lookup"><span data-stu-id="ddc3f-210">*MINIMUM_SUPPORT*</span></span>  
 <span data-ttu-id="ddc3f-211">Determina el número mínimo de casos de hoja necesarios para generar una división en el árbol de decisión.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-211">Determines the minimum number of leaf cases that is required to generate a split in the decision tree.</span></span>  
  
 <span data-ttu-id="ddc3f-212">El valor predeterminado es 10.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-212">The default is 10.</span></span>  
  
 <span data-ttu-id="ddc3f-213">Es posible que necesite aumentar este valor si el conjunto de datos es muy grande, para evitar el sobreentrenamiento.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-213">You may need to increase this value if the dataset is very large, to avoid overtraining.</span></span>  
  
 <span data-ttu-id="ddc3f-214">*SCORE_METHOD*</span><span class="sxs-lookup"><span data-stu-id="ddc3f-214">*SCORE_METHOD*</span></span>  
 <span data-ttu-id="ddc3f-215">Determina el método usado para calcular el resultado de la división.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-215">Determines the method that is used to calculate the split score.</span></span> <span data-ttu-id="ddc3f-216">Están disponibles las siguientes opciones:</span><span class="sxs-lookup"><span data-stu-id="ddc3f-216">The following options are available:</span></span>  
  
|<span data-ttu-id="ddc3f-217">ID</span><span class="sxs-lookup"><span data-stu-id="ddc3f-217">ID</span></span>|<span data-ttu-id="ddc3f-218">Nombre</span><span class="sxs-lookup"><span data-stu-id="ddc3f-218">Name</span></span>|  
|--------|----------|  
|<span data-ttu-id="ddc3f-219">1</span><span class="sxs-lookup"><span data-stu-id="ddc3f-219">1</span></span>|<span data-ttu-id="ddc3f-220">Entropía</span><span class="sxs-lookup"><span data-stu-id="ddc3f-220">Entropy</span></span>|  
|<span data-ttu-id="ddc3f-221">3</span><span class="sxs-lookup"><span data-stu-id="ddc3f-221">3</span></span>|<span data-ttu-id="ddc3f-222">Bayesiano con prioridad K2</span><span class="sxs-lookup"><span data-stu-id="ddc3f-222">Bayesian with K2 Prior</span></span>|  
|<span data-ttu-id="ddc3f-223">4</span><span class="sxs-lookup"><span data-stu-id="ddc3f-223">4</span></span>|<span data-ttu-id="ddc3f-224">Equivalente Dirichlet bayesiano (BDE) con prioridad uniforme</span><span class="sxs-lookup"><span data-stu-id="ddc3f-224">Bayesian Dirichlet Equivalent (BDE) with uniform prior</span></span><br /><br /> <span data-ttu-id="ddc3f-225">(predeterminado).</span><span class="sxs-lookup"><span data-stu-id="ddc3f-225">(default)</span></span>|  
  
 <span data-ttu-id="ddc3f-226">El valor predeterminado es 4 o BDE.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-226">The default is 4, or BDE.</span></span>  
  
 <span data-ttu-id="ddc3f-227">Para obtener una explicación de estos métodos de puntuación, vea [Feature Selection](../../sql-server/install/feature-selection.md).</span><span class="sxs-lookup"><span data-stu-id="ddc3f-227">For an explanation of these scoring methods, see [Feature Selection](../../sql-server/install/feature-selection.md).</span></span>  
  
 <span data-ttu-id="ddc3f-228">*SPLIT_METHOD*</span><span class="sxs-lookup"><span data-stu-id="ddc3f-228">*SPLIT_METHOD*</span></span>  
 <span data-ttu-id="ddc3f-229">Determina el método usado para dividir el nodo.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-229">Determines the method that is used to split the node.</span></span> <span data-ttu-id="ddc3f-230">Están disponibles las siguientes opciones:</span><span class="sxs-lookup"><span data-stu-id="ddc3f-230">The following options are available:</span></span>  
  
|<span data-ttu-id="ddc3f-231">ID</span><span class="sxs-lookup"><span data-stu-id="ddc3f-231">ID</span></span>|<span data-ttu-id="ddc3f-232">Nombre</span><span class="sxs-lookup"><span data-stu-id="ddc3f-232">Name</span></span>|  
|--------|----------|  
|<span data-ttu-id="ddc3f-233">1</span><span class="sxs-lookup"><span data-stu-id="ddc3f-233">1</span></span>|<span data-ttu-id="ddc3f-234">**Binary:** indica que, independientemente del número real de valores para el atributo, el árbol se debería dividir en dos bifurcaciones.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-234">**Binary:** Indicates that regardless of the actual number of values for the attribute, the tree should be split into two branches.</span></span>|  
|<span data-ttu-id="ddc3f-235">2</span><span class="sxs-lookup"><span data-stu-id="ddc3f-235">2</span></span>|<span data-ttu-id="ddc3f-236">**Complete:** indica que el árbol puede crear tantas divisiones como valores de atributo existan.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-236">**Complete:** Indicates that the tree can create as many splits as there are attribute values.</span></span>|  
|<span data-ttu-id="ddc3f-237">3</span><span class="sxs-lookup"><span data-stu-id="ddc3f-237">3</span></span>|<span data-ttu-id="ddc3f-238">**Both:** especifica que Analysis Services puede determinar si se debe usar una división binaria o completa para generar los mejores resultados.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-238">**Both:** Specifies that Analysis Services can determine whether a binary or complete split should be used to produce the best results.</span></span>|  
  
 <span data-ttu-id="ddc3f-239">El valor predeterminado es 3.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-239">The default is 3.</span></span>  
  
### <a name="modeling-flags"></a><span data-ttu-id="ddc3f-240">Marcas de modelado</span><span class="sxs-lookup"><span data-stu-id="ddc3f-240">Modeling Flags</span></span>  
 <span data-ttu-id="ddc3f-241">El algoritmo de árboles de decisión de [!INCLUDE[msCoName](../../includes/msconame-md.md)] admite las marcas de modelado siguientes.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-241">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm supports the following modeling flags.</span></span> <span data-ttu-id="ddc3f-242">Al crear la estructura o el modelo de minería de datos, se definen las marcas de modelado que especifican cómo se tratan los valores de cada columna durante el análisis.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-242">When you create the mining structure or mining model, you define modeling flags to specify how values in each column are handled during analysis.</span></span> <span data-ttu-id="ddc3f-243">Para obtener más información, vea [Marcas de modelado &#40;Minería de datos&#41;](modeling-flags-data-mining.md).</span><span class="sxs-lookup"><span data-stu-id="ddc3f-243">For more information, see [Modeling Flags &#40;Data Mining&#41;](modeling-flags-data-mining.md).</span></span>  
  
|<span data-ttu-id="ddc3f-244">Marca de modelado</span><span class="sxs-lookup"><span data-stu-id="ddc3f-244">Modeling Flag</span></span>|<span data-ttu-id="ddc3f-245">Descripción</span><span class="sxs-lookup"><span data-stu-id="ddc3f-245">Description</span></span>|  
|-------------------|-----------------|  
|<span data-ttu-id="ddc3f-246">MODEL_EXISTENCE_ONLY</span><span class="sxs-lookup"><span data-stu-id="ddc3f-246">MODEL_EXISTENCE_ONLY</span></span>|<span data-ttu-id="ddc3f-247">Significa que la columna se tratará como si tuviera dos estados posibles: `Missing` y `Existing`.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-247">Means that the column will be treated as having two possible states: `Missing` and `Existing`.</span></span> <span data-ttu-id="ddc3f-248">Un valor NULL es un valor ausente.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-248">A null is a missing value.</span></span><br /><br /> <span data-ttu-id="ddc3f-249">Se aplica a las columnas del modelo de minería de datos.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-249">Applies to mining model columns.</span></span>|  
|<span data-ttu-id="ddc3f-250">NOT NULL</span><span class="sxs-lookup"><span data-stu-id="ddc3f-250">NOT NULL</span></span>|<span data-ttu-id="ddc3f-251">Indica que la columna no puede contener un valor NULL.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-251">Indicates that the column cannot contain a null.</span></span> <span data-ttu-id="ddc3f-252">Se producirá un error si Analysis Services encuentra un valor NULL durante el entrenamiento del modelo.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-252">An error will result if Analysis Services encounters a null during model training.</span></span><br /><br /> <span data-ttu-id="ddc3f-253">Se aplica a las columnas de la estructura de minería de datos.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-253">Applies to mining structure columns.</span></span>|  
  
### <a name="regressors-in-decision-tree-models"></a><span data-ttu-id="ddc3f-254">Regresores en modelos de árbol de decisión</span><span class="sxs-lookup"><span data-stu-id="ddc3f-254">Regressors in Decision Tree Models</span></span>  
 <span data-ttu-id="ddc3f-255">Aun cuando no use el algoritmo de regresión lineal de [!INCLUDE[msCoName](../../includes/msconame-md.md)] , cualquier modelo de árbol de decisión que tenga entradas y salidas numéricas continuas puede incluir nodos que representan una regresión en un atributo continuo.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-255">Even if you do not use the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Linear Regression algorithm, any decision tree model that has continuous numeric inputs and outputs can potentially include nodes that represent a regression on a continuous attribute.</span></span>  
  
 <span data-ttu-id="ddc3f-256">No es necesario especificar que una columna de datos numéricos continuos representa un regresor.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-256">You do not need to specify that a column of continuous numeric data represents a regressor.</span></span> <span data-ttu-id="ddc3f-257">El algoritmo de árboles de decisión de [!INCLUDE[msCoName](../../includes/msconame-md.md)] usará automáticamente la columna como un regresor potencial y dividirá el conjunto de datos en regiones con patrones significativos aunque no se establezca la marca REGRESSOR en la columna.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-257">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm will automatically use the column as a potential regressor and partition the dataset into regions with meaningful patterns even if you do not set the REGRESSOR flag on the column.</span></span>  
  
 <span data-ttu-id="ddc3f-258">Sin embargo, puede usar el parámetro FORCE_REGRESSOR para garantizar que el algoritmo empleará un regresor determinado.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-258">However, you can use the FORCE_REGRESSOR parameter to guarantee that the algorithm will use a particular regressor.</span></span> <span data-ttu-id="ddc3f-259">Este parámetro solo se puede usar con los algoritmos de árboles de decisión de [!INCLUDE[msCoName](../../includes/msconame-md.md)] y de regresión lineal de [!INCLUDE[msCoName](../../includes/msconame-md.md)] .</span><span class="sxs-lookup"><span data-stu-id="ddc3f-259">This parameter can be used only with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees and [!INCLUDE[msCoName](../../includes/msconame-md.md)] Linear Regression algorithms.</span></span> <span data-ttu-id="ddc3f-260">Al establecer la marca de modelado, el algoritmo intentará buscar ecuaciones de regresión con el formato a \* C1 + b \* C2 +... para ajustarse a los patrones de los nodos del árbol.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-260">When you set the modeling flag, the algorithm will try to find regression equations of the form a\*C1 + b\*C2 + ... to fit the patterns in the nodes of the tree.</span></span> <span data-ttu-id="ddc3f-261">Se calcula la suma de los valores residuales y, si la desviación es demasiado grande, se fuerza una división en el árbol.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-261">The sum of the residuals is calculated, and if the deviation is too great, a split is forced in the tree.</span></span>  
  
 <span data-ttu-id="ddc3f-262">Por ejemplo, si está prediciendo los hábitos de compra de los clientes usando **Income** como atributo y ha establecido la marca de modelado REGRESSOR en la columna, el algoritmo intentará en primer lugar ajustar los valores de **Income** mediante una fórmula de regresión estándar.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-262">For example, if you are predicting customer purchasing behavior using **Income** as an attribute, and set the REGRESSOR modeling flag on the column, the algorithm will first try to fit the **Income** values by using a standard regression formula.</span></span> <span data-ttu-id="ddc3f-263">Si la desviación es demasiado grande, se abandona la fórmula de regresión y el árbol se dividirá de acuerdo con otro atributo.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-263">If the deviation is too great, the regression formula is abandoned and the tree will be split on another attribute.</span></span> <span data-ttu-id="ddc3f-264">A continuación, el algoritmo de árboles de decisión intentará ajustar un regresor para los ingresos en cada una de las ramas después de la división.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-264">The decision tree algorithm will then try to fit a regressor for income in each of the branches after the split.</span></span>  
  
## <a name="requirements"></a><span data-ttu-id="ddc3f-265">Requisitos</span><span class="sxs-lookup"><span data-stu-id="ddc3f-265">Requirements</span></span>  
 <span data-ttu-id="ddc3f-266">Un modelo de árbol de decisión debe contener una columna de clave, columnas de entrada y al menos una columna de predicción.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-266">A decision tree model must contain a key column, input columns, and at least one predictable column.</span></span>  
  
### <a name="input-and-predictable-columns"></a><span data-ttu-id="ddc3f-267">Columnas de entrada y de predicción</span><span class="sxs-lookup"><span data-stu-id="ddc3f-267">Input and Predictable Columns</span></span>  
 <span data-ttu-id="ddc3f-268">El algoritmo de árboles de decisión de [!INCLUDE[msCoName](../../includes/msconame-md.md)] admite las columnas de entrada y de predicción específicas que se incluyen en la tabla siguiente.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-268">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm supports the specific input columns and predictable columns that are listed in the following table.</span></span> <span data-ttu-id="ddc3f-269">Para más información sobre el significado de los tipos de contenido usados en un modelo de minería de datos, vea [Tipos de contenido &#40;minería de datos&#41;](content-types-data-mining.md).</span><span class="sxs-lookup"><span data-stu-id="ddc3f-269">For more information about what the content types mean when used in a mining model, see [Content Types &#40;Data Mining&#41;](content-types-data-mining.md).</span></span>  
  
|<span data-ttu-id="ddc3f-270">Columna</span><span class="sxs-lookup"><span data-stu-id="ddc3f-270">Column</span></span>|<span data-ttu-id="ddc3f-271">Tipos de contenido</span><span class="sxs-lookup"><span data-stu-id="ddc3f-271">Content types</span></span>|  
|------------|-------------------|  
|<span data-ttu-id="ddc3f-272">Atributo de entrada</span><span class="sxs-lookup"><span data-stu-id="ddc3f-272">Input attribute</span></span>|<span data-ttu-id="ddc3f-273">Continuous, Cyclical, Discrete, Discretized, Key, Ordered, Table</span><span class="sxs-lookup"><span data-stu-id="ddc3f-273">Continuous, Cyclical, Discrete, Discretized, Key, Ordered, Table</span></span>|  
|<span data-ttu-id="ddc3f-274">Atributo de predicción</span><span class="sxs-lookup"><span data-stu-id="ddc3f-274">Predictable attribute</span></span>|<span data-ttu-id="ddc3f-275">Continuous, Cyclical, Discrete, Discretized, Ordered, Table</span><span class="sxs-lookup"><span data-stu-id="ddc3f-275">Continuous, Cyclical, Discrete, Discretized, Ordered, Table</span></span>|  
  
> [!NOTE]  
>  <span data-ttu-id="ddc3f-276">Se admiten los tipos de contenido Cyclical y Ordered, pero el algoritmo los trata como valores discretos y no realiza un procesamiento especial.</span><span class="sxs-lookup"><span data-stu-id="ddc3f-276">Cyclical and Ordered content types are supported, but the algorithm treats them as discrete values and does not perform special processing.</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="ddc3f-277">Consulte también</span><span class="sxs-lookup"><span data-stu-id="ddc3f-277">See Also</span></span>  
 <span data-ttu-id="ddc3f-278">[Algoritmo de árboles de decisión de Microsoft](microsoft-decision-trees-algorithm.md) </span><span class="sxs-lookup"><span data-stu-id="ddc3f-278">[Microsoft Decision Trees Algorithm](microsoft-decision-trees-algorithm.md) </span></span>  
 <span data-ttu-id="ddc3f-279">[Ejemplos de consultas de modelos de árboles de decisión](decision-trees-model-query-examples.md) </span><span class="sxs-lookup"><span data-stu-id="ddc3f-279">[Decision Trees Model Query Examples](decision-trees-model-query-examples.md) </span></span>  
 [<span data-ttu-id="ddc3f-280">Contenido del modelo de minería de datos para los modelos de árboles de decisión &#40;Analysis Services - Minería de datos&#41;</span><span class="sxs-lookup"><span data-stu-id="ddc3f-280">Mining Model Content for Decision Tree Models &#40;Analysis Services - Data Mining&#41;</span></span>](mining-model-content-for-decision-tree-models-analysis-services-data-mining.md)  
  
  
